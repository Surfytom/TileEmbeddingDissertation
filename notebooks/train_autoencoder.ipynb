{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44679ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.preprocessing.image import array_to_img, save_img, img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras import metrics\n",
    "from keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Dropout,\n",
    "    UpSampling2D,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "import pickle\n",
    "from keras.layers import ReLU, Reshape, Conv2DTranspose, Concatenate, Multiply\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from collections import Counter\n",
    "from keras import metrics\n",
    "from notebook_utils import initialize_environment\n",
    "initialize_environment()\n",
    "\n",
    "from utils.data_loading.load_data import get_tile_data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc52985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../data/context_data/\"\n",
    "json_directory = \"../data/json_files_trimmed_features/\"\n",
    "\n",
    "game_data_path=\"../data/game_data.csv\"\n",
    "train_data_path=\"../data/train_data.csv\"\n",
    "test_data_path=\"../data/test_data.csv\"\n",
    "\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=25\n",
    "LATENT_DIM=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_image(y_true, y_pred):\n",
    "    # tile sprite loss\n",
    "    r_loss=K.mean(K.square(y_true - y_pred), axis=[1,2,3])\n",
    "    loss  =  r_loss\n",
    "    return loss\n",
    "    \n",
    "def loss_func_text(y_true,y_pred):\n",
    "    # multilabel text weighted bce\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    bce_array=-(y_true*K.log(y_pred)+(1-y_true)*K.log(1-y_pred))\n",
    "    weighted_array=bce_array*tensor_from_list\n",
    "    bce_sum=K.sum(weighted_array,axis=1)\n",
    "    loss=bce_sum/13.0\n",
    "    return loss\n",
    "\n",
    "def check_nonzero(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Custom metric\n",
    "    Returns sum of all embeddings\n",
    "    \"\"\"\n",
    "    return(K.sum(K.cast(y_pred > 0.4, 'int32')))\n",
    "\n",
    "def get_pickle_file(path):\n",
    "    with open(path,\"rb\") as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ec3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dictionary Loaded\n",
      "The feature dictionary has size 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Features'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['block', 'breakable', 'climbable', 'collectable', 'element',\n",
       "       'empty', 'hazard', 'moving', 'openable', 'passable', 'pipe',\n",
       "       'solid', 'wall'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Training Testing Batches loaded\n",
      "Train Image batch shape (22854, 48, 48, 3)\n",
      "Train Text batch shape (22854, 13)\n",
      "Train Output Image batch shape (22854, 16, 16, 3)\n",
      "Train Output Text batch shape (22854, 13)\n",
      "Test Image batch shape (2540, 48, 48, 3)\n",
      "Test Text batch shape (2540, 13)\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "data=pd.read_csv(game_data_path)\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "test_data=pd.read_csv(test_data_path)\n",
    "\n",
    "data['features'] = data.features.apply(lambda x: literal_eval(str(x)))\n",
    "train_data['features'] = train_data.features.apply(lambda x: literal_eval(str(x)))\n",
    "test_data['features'] = test_data.features.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "# loading multi-label binarizer\n",
    "mlb=get_pickle_file(\"../model/model_tokenizer.pickle\")\n",
    "print(\"Feature Dictionary Loaded\")\n",
    "total_features = len(mlb.classes_)\n",
    "print(\"The feature dictionary has size\", total_features)\n",
    "display(\"Features\", mlb.classes_)\n",
    "\n",
    "# loading the batches\n",
    "# training \n",
    "train_image_batch=get_pickle_file(\"../data/train_image_batch.pickle\")\n",
    "train_text_batch=get_pickle_file(\"../data/train_text_batch.pickle\")\n",
    "output_image_batch=get_pickle_file(\"../data/output_image_batch.pickle\")\n",
    "output_text_batch=get_pickle_file(\"../data/output_text_batch.pickle\")\n",
    "\n",
    "#testing\n",
    "test_image_batch=get_pickle_file(\"../data/test_image_batch.pickle\")\n",
    "test_text_batch=get_pickle_file(\"../data/test_text_batch.pickle\")\n",
    "\n",
    "print(\"\\Training Testing Batches loaded\")\n",
    "\n",
    "print(\"Train Image batch shape\", train_image_batch.shape)\n",
    "print(\"Train Text batch shape\", train_text_batch.shape)\n",
    "print(\"Train Output Image batch shape\", output_image_batch.shape)\n",
    "print(\"Train Output Text batch shape\", output_text_batch.shape)\n",
    "\n",
    "print(\"Test Image batch shape\", test_image_batch.shape)\n",
    "print(\"Test Text batch shape\", test_text_batch.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476bdaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Printing the TF-IDF for the labels\n",
      "\n",
      " {'block': 5.08368185647806, 'breakable': 2.5970451024522774, 'climbable': 2.761548815929438, 'collectable': 4.099611109542163, 'element': 5.56465451709437, 'empty': 2.0965579574606696, 'hazard': 4.8084141871746615, 'moving': 6.184894926846227, 'openable': 6.411952377481573, 'passable': 1.603201296270845, 'pipe': 6.53711552043558, 'solid': 1.827785539366181, 'wall': 5.268604194972072}\n",
      "\n",
      "Printing the weight normalised\n",
      "\n",
      "\n",
      "{'block': 0.09269168764552638, 'breakable': 0.04735239148199231, 'climbable': 0.050351817342350516, 'collectable': 0.07474894833349648, 'element': 0.10146134886401398, 'empty': 0.038226919152316914, 'hazard': 0.08767268261289057, 'moving': 0.11277030405612387, 'openable': 0.11691028348167781, 'passable': 0.029231458219100986, 'pipe': 0.1191924056283738, 'solid': 0.033326343205771214, 'wall': 0.0960634099763652}\n",
      "Weight Vector\n",
      "[5083.681856478061, 2597.045102452277, 2761.548815929438, 4099.611109542163, 5564.65451709437, 2096.5579574606695, 4808.4141871746615, 6184.8949268462275, 6411.952377481573, 1603.201296270845, 6537.11552043558, 1827.785539366181, 5268.604194972072]\n"
     ]
    }
   ],
   "source": [
    "#initialise the TF-IDF vectorizer to counter imbalanced dataset\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "train_data_copy = train_data\n",
    "train_data_copy[\"features\"] = train_data_copy.features.apply(lambda x: str(x))\n",
    "vectors = vectorizer.fit_transform(train_data_copy[\"features\"])\n",
    "idf = vectorizer.idf_\n",
    "new_dict = {}\n",
    "for c in mlb.classes_:\n",
    "    if c in vectorizer.vocabulary_.keys():\n",
    "        new_dict[c] = idf[vectorizer.vocabulary_[c]]\n",
    "    else:\n",
    "        new_dict[c] = np.max(idf)\n",
    "print(\"\\n Printing the TF-IDF for the labels\\n\\n\", new_dict)\n",
    "weight_freq = {k: v / sum(new_dict.values()) for k, v in new_dict.items()}\n",
    "print(\"\\nPrinting the weight normalised\\n\\n\")\n",
    "print(weight_freq)\n",
    "weight_vector = [v * 1000 for v in new_dict.values()]\n",
    "tensor_from_list = tf.convert_to_tensor(weight_vector)\n",
    "tensor_from_list = K.cast(tensor_from_list, \"float32\")\n",
    "print(\"Weight Vector\")\n",
    "print(weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b240e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "latent_dim = 128\n",
    "batch_size = 1\n",
    "# image encoder\n",
    "image_encoder_input = Input(shape=(48, 48, 3), name=\"image_input\")\n",
    "image_encoder_conv_layer1 = Conv2D(\n",
    "    32, strides=3, kernel_size=(3, 3), name=\"iencode_conv1\"\n",
    ")(image_encoder_input)\n",
    "image_encoder_norm_layer1 = BatchNormalization()(image_encoder_conv_layer1)\n",
    "image_encoder_actv_layer1 = ReLU()(image_encoder_norm_layer1)\n",
    "image_encoder_conv_layer2 = Conv2D(32, (3, 3), padding=\"same\", name=\"iencode_conv2\")(\n",
    "    image_encoder_actv_layer1\n",
    ")\n",
    "image_encoder_norm_layer2 = BatchNormalization()(image_encoder_conv_layer2)\n",
    "image_encoder_actv_layer2 = ReLU()(image_encoder_norm_layer2)\n",
    "image_encoder_conv_layer3 = Conv2D(16, (3, 3), padding=\"same\", name=\"iencode_conv3\")(\n",
    "    image_encoder_actv_layer2\n",
    ")\n",
    "image_encoder_norm_layer3 = BatchNormalization()(image_encoder_conv_layer3)\n",
    "image_encoder_actv_layer3 = ReLU()(image_encoder_norm_layer3)\n",
    "image_shape_before_flatten = K.int_shape(image_encoder_actv_layer3)[1:]\n",
    "image_flatten = Flatten(name=\"image_flatten_layer\")(image_encoder_actv_layer3)\n",
    "\n",
    "# text encoder\n",
    "text_encoder_input = Input(shape=(13,))\n",
    "text_encoder_dense_layer1 = Dense(32, activation=\"tanh\", name=\"tencode_dense1\")(\n",
    "    text_encoder_input\n",
    ")\n",
    "text_encoder_dense_layer2 = Dense(16, activation=\"tanh\", name=\"tencode_dense2\")(\n",
    "    text_encoder_dense_layer1\n",
    ")\n",
    "text_shape_before_concat = K.int_shape(text_encoder_dense_layer2)[1:]\n",
    "\n",
    "# image-text concatenation\n",
    "image_text_concat = Concatenate(name=\"image_text_concatenation\")(\n",
    "    [image_flatten, text_encoder_dense_layer2]\n",
    ")\n",
    "image_text_concat = Dense(256, activation=\"tanh\", name=\"embedding_dense_1\")(\n",
    "    image_text_concat\n",
    ")\n",
    "\n",
    "\n",
    "# define encoder model\n",
    "encoding_model = Model(\n",
    "    inputs=[image_encoder_input, text_encoder_input], outputs=image_text_concat\n",
    ")\n",
    "\n",
    "# decoder for image\n",
    "# decoder_input=Input(shape=(512,))\n",
    "image_y = Dense(units=np.prod(image_shape_before_flatten), name=\"image_dense\")(\n",
    "    image_text_concat\n",
    ")\n",
    "image_y = Reshape(target_shape=image_shape_before_flatten, name=\"image_reshape\")(\n",
    "    image_y\n",
    ")\n",
    "image_decoder_convt_layer1 = Conv2DTranspose(\n",
    "    16, (3, 3), padding=\"same\", name=\"idecode_conv1\"\n",
    ")(image_y)\n",
    "image_decoder_norm_layer1 = BatchNormalization(name=\"idecode_norm1\")(\n",
    "    image_decoder_convt_layer1\n",
    ")\n",
    "image_decoder_actv_layer1 = ReLU(name=\"idecode_relu1\")(image_decoder_norm_layer1)\n",
    "image_decoder_convt_layer2 = Conv2DTranspose(\n",
    "    32, (3, 3), padding=\"same\", name=\"idecode_conv2\"\n",
    ")(image_decoder_actv_layer1)\n",
    "image_decoder_norm_layer2 = BatchNormalization(name=\"idecode_norm2\")(\n",
    "    image_decoder_convt_layer2\n",
    ")\n",
    "image_decoder_actv_layer2 = ReLU(name=\"idecode_relu2\")(image_decoder_norm_layer2)\n",
    "image_decoder_output = Conv2DTranspose(\n",
    "    3, (3, 3), padding=\"same\", name=\"image_output_layer\"\n",
    ")(image_decoder_actv_layer2)\n",
    "\n",
    "\n",
    "# decoder for text\n",
    "text_decoder_dense_layer1 = Dense(16, activation=\"tanh\", name=\"tdecode_dense1\")(\n",
    "    image_text_concat\n",
    ")\n",
    "text_reshape = Reshape(target_shape=text_shape_before_concat, name=\"text_reshape\")(\n",
    "    text_decoder_dense_layer1\n",
    ")\n",
    "text_decoder_dense_layer2 = Dense(32, activation=\"tanh\", name=\"tdecode_dense2\")(\n",
    "    text_reshape\n",
    ")\n",
    "text_decoder_output = Dense(13, activation=\"sigmoid\", name=\"text_output_layer\")(\n",
    "    text_decoder_dense_layer2\n",
    ")\n",
    "# decoding_model=Model(inputs=[decoder_input],outputs=[image_decoder_output,text_decoder_output])\n",
    "\n",
    "ae_sep_output = Model(\n",
    "    [image_encoder_input, text_encoder_input],\n",
    "    [image_decoder_output, text_decoder_output],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b877ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "732/732 [==============================] - 16s 20ms/step - loss: 891.0165 - image_output_layer_loss: 4600.9644 - text_output_layer_loss: 478.8000 - image_output_layer_loss_func_image: 4600.9644 - text_output_layer_check_nonzero: 45.7732 - val_loss: 378.1501 - val_image_output_layer_loss: 1854.3604 - val_text_output_layer_loss: 214.1268 - val_image_output_layer_loss_func_image: 1854.3604 - val_text_output_layer_check_nonzero: 44.2896\n",
      "Epoch 2/10\n",
      "732/732 [==============================] - 13s 18ms/step - loss: 253.7060 - image_output_layer_loss: 1446.2452 - text_output_layer_loss: 121.2020 - image_output_layer_loss_func_image: 1446.2452 - text_output_layer_check_nonzero: 45.0014 - val_loss: 191.2618 - val_image_output_layer_loss: 1129.7433 - val_text_output_layer_loss: 86.9860 - val_image_output_layer_loss_func_image: 1129.7433 - val_text_output_layer_check_nonzero: 45.2076\n",
      "Epoch 3/10\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 155.9269 - image_output_layer_loss: 1061.9918 - text_output_layer_loss: 55.2532 - image_output_layer_loss_func_image: 1061.9918 - text_output_layer_check_nonzero: 45.3484 - val_loss: 128.7104 - val_image_output_layer_loss: 825.3015 - val_text_output_layer_loss: 51.3114 - val_image_output_layer_loss_func_image: 825.3015 - val_text_output_layer_check_nonzero: 45.2022\n",
      "Epoch 4/10\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 110.2156 - image_output_layer_loss: 823.1223 - text_output_layer_loss: 31.0037 - image_output_layer_loss_func_image: 823.1223 - text_output_layer_check_nonzero: 45.2555 - val_loss: 94.5792 - val_image_output_layer_loss: 682.4896 - val_text_output_layer_loss: 29.2559 - val_image_output_layer_loss_func_image: 682.4896 - val_text_output_layer_check_nonzero: 45.2186\n",
      "Epoch 5/10\n",
      "732/732 [==============================] - 14s 19ms/step - loss: 82.8598 - image_output_layer_loss: 666.1251 - text_output_layer_loss: 18.0526 - image_output_layer_loss_func_image: 666.1251 - text_output_layer_check_nonzero: 45.3716 - val_loss: 75.9671 - val_image_output_layer_loss: 552.2949 - val_text_output_layer_loss: 23.0418 - val_image_output_layer_loss_func_image: 552.2949 - val_text_output_layer_check_nonzero: 45.4262\n",
      "Epoch 6/10\n",
      "228/732 [========>.....................] - ETA: 8s - loss: 69.0583 - image_output_layer_loss: 580.4731 - text_output_layer_loss: 12.2344 - image_output_layer_loss_func_image: 580.4731 - text_output_layer_check_nonzero: 45.5088"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ed6fe051ad1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m )\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/TileEmbeddingDissertation-Ir-yPq9b/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses ={'image_output_layer':loss_func_image,\n",
    "          'text_output_layer':loss_func_text,\n",
    "}\n",
    "#tweak loss weights\n",
    "lossWeights={'image_output_layer':0.1,\n",
    "          'text_output_layer':0.9  \n",
    "        }\n",
    "\n",
    "\n",
    "accuracy={\n",
    "    'image_output_layer':loss_func_image,\n",
    "    'text_output_layer': check_nonzero\n",
    "}\n",
    "\n",
    "ae_sep_output.compile(\n",
    "    optimizer=\"adam\", loss=losses, loss_weights=lossWeights, metrics=accuracy\n",
    ")\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_text_output_layer_loss\", mode=\"min\", verbose=1, patience=2\n",
    ")\n",
    "\n",
    "ae_history = ae_sep_output.fit(\n",
    "    [train_image_batch, train_text_batch],\n",
    "    [output_image_batch, output_text_batch],\n",
    "    epochs=10,\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a716e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 51.9090 - image_output_layer_loss: 465.7723 - text_output_layer_loss: 5.9241 - image_output_layer_loss_func_image: 465.7723 - text_output_layer_check_nonzero: 46.6723 - val_loss: 49.7016 - val_image_output_layer_loss: 436.4905 - val_text_output_layer_loss: 6.7251 - val_image_output_layer_loss_func_image: 436.4905 - val_text_output_layer_check_nonzero: 46.5473\n",
      "Epoch 2/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 47.1144 - image_output_layer_loss: 423.9874 - text_output_layer_loss: 5.2397 - image_output_layer_loss_func_image: 423.9874 - text_output_layer_check_nonzero: 46.6723 - val_loss: 55.2789 - val_image_output_layer_loss: 441.1127 - val_text_output_layer_loss: 12.4084 - val_image_output_layer_loss_func_image: 441.1127 - val_text_output_layer_check_nonzero: 46.4392\n",
      "Epoch 3/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 55.8166 - image_output_layer_loss: 480.3064 - text_output_layer_loss: 8.6511 - image_output_layer_loss_func_image: 480.3064 - text_output_layer_check_nonzero: 46.6334 - val_loss: 41.5654 - val_image_output_layer_loss: 359.3868 - val_text_output_layer_loss: 6.2519 - val_image_output_layer_loss_func_image: 359.3868 - val_text_output_layer_check_nonzero: 46.5405\n",
      "Epoch 4/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 41.7013 - image_output_layer_loss: 370.4573 - text_output_layer_loss: 5.1729 - image_output_layer_loss_func_image: 370.4573 - text_output_layer_check_nonzero: 46.6115 - val_loss: 60.9125 - val_image_output_layer_loss: 556.4233 - val_text_output_layer_loss: 5.8558 - val_image_output_layer_loss_func_image: 556.4234 - val_text_output_layer_check_nonzero: 46.4662\n",
      "Epoch 5/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 36.9564 - image_output_layer_loss: 340.3987 - text_output_layer_loss: 3.2406 - image_output_layer_loss_func_image: 340.3987 - text_output_layer_check_nonzero: 46.5997 - val_loss: 35.6880 - val_image_output_layer_loss: 327.4654 - val_text_output_layer_loss: 3.2683 - val_image_output_layer_loss_func_image: 327.4654 - val_text_output_layer_check_nonzero: 46.4189\n"
     ]
    }
   ],
   "source": [
    "ae_history = ae_sep_output.fit(\n",
    "    [train_image_batch, train_text_batch],\n",
    "    [output_image_batch, output_text_batch],\n",
    "    epochs=5,\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc34a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the inference model\n",
    "\n",
    "decoder_input = Input(shape=(256,))\n",
    "\n",
    "d_dense = ae_sep_output.get_layer(\"image_dense\")(decoder_input)\n",
    "d_reshape = ae_sep_output.get_layer(\"image_reshape\")(d_dense)\n",
    "d_conv1 = ae_sep_output.get_layer(\"idecode_conv1\")(d_reshape)\n",
    "d_norm1 = ae_sep_output.get_layer(\"idecode_norm1\")(d_conv1)\n",
    "d_relu1 = ae_sep_output.get_layer(\"idecode_relu1\")(d_norm1)\n",
    "d_conv2 = ae_sep_output.get_layer(\"idecode_conv2\")(d_relu1)\n",
    "d_norm2 = ae_sep_output.get_layer(\"idecode_norm2\")(d_conv2)\n",
    "d_relu2 = ae_sep_output.get_layer(\"idecode_relu2\")(d_norm2)\n",
    "d_image_output = ae_sep_output.get_layer(\"image_output_layer\")(d_relu2)\n",
    "\n",
    "t_dense = ae_sep_output.get_layer(\"tdecode_dense1\")(decoder_input)\n",
    "t_reshape = ae_sep_output.get_layer(\"text_reshape\")(t_dense)\n",
    "t_dense2 = ae_sep_output.get_layer(\"tdecode_dense2\")(t_reshape)\n",
    "d_text_output = ae_sep_output.get_layer(\"text_output_layer\")(t_dense2)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_input], outputs=[d_image_output, d_text_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca7d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Entire Model to disk\n",
      "Saved Encoder Model to disk\n",
      "Saved Decoder Model to disk\n"
     ]
    }
   ],
   "source": [
    "# saving the entire architecture model\n",
    "model_json = ae_sep_output.to_json()\n",
    "with open(\"../model/autoencoder_model_test_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "ae_sep_output.save_weights(\"../model/autoencoder_model_test_2.h5\")\n",
    "print(\"Saved Entire Model to disk\")\n",
    "\n",
    "# saving the encoder part\n",
    "model_json = encoding_model.to_json()\n",
    "with open(\"../model/encoder_model_test_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "encoding_model.save_weights(\"../model/encoder_model_test_2.h5\")\n",
    "print(\"Saved Encoder Model to disk\")\n",
    "# saving the encoder part\n",
    "\n",
    "model_json = decoder_model.to_json()\n",
    "with open(\"../model/decoder_model_test_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "decoder_model.save_weights(\"../model/decoder_model_test_2.h5\")\n",
    "print(\"Saved Decoder Model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
