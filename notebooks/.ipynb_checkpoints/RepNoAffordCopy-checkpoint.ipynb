{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.preprocessing.image import load_img, save_img\n",
    "from keras.preprocessing.image import array_to_img, img_to_array\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from collections import Counter\n",
    "from utils.data_loading.load_data import get_tile_data\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b44e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec066f800773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the multilabel binarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../model/model_tokenizer.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature Dictionary Loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 5"
     ]
    }
   ],
   "source": [
    "# load the multilabel binarizer\n",
    "with open(\"../model/model_tokenizer.pickle\", \"rb\") as handle:\n",
    "    mlb = pickle.load(handle)\n",
    "print(\"Feature Dictionary Loaded\")\n",
    "total_features = len(mlb.classes_)\n",
    "print(\"The feature dictionary has size\", total_features)\n",
    "print(\"Features\", mlb.classes_)\n",
    "\n",
    "# load entire autoencoder architecture\n",
    "json_file = open(\"../model/autoencoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "ae_sep_output = model_from_json(loaded_model_json)\n",
    "ae_sep_output.load_weights(\"../model/autoencoder_model_test.h5\")\n",
    "print(\"Loaded Entire Autoencoder Model from the Disk\")\n",
    "\n",
    "# load the encoding architecture and weights\n",
    "json_file = open(\"../model/encoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "encoding_model = model_from_json(loaded_model_json)\n",
    "encoding_model.load_weights(\"../model/encoder_model_test.h5\")\n",
    "print(\"Loaded Encoder Model from the Disk\")\n",
    "\n",
    "# load the decoding architecture and weights\n",
    "json_file = open(\"../model/decoder_model_test.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "decoding_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "decoding_model.load_weights(\"../model/decoder_model_test.h5\")\n",
    "print(\"Loaded Decoder Model from the Disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392f38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    img_without_border = load_img(path)\n",
    "    img = Image.open(path)\n",
    "    img_with_border = ImageOps.expand(img_without_border, border=16, fill=\"black\")\n",
    "    return img_without_border, img_with_border\n",
    "\n",
    "def level_image_unroll(level_array_padded):\n",
    "    level_image_unrolled = []\n",
    "    image_h, image_w, image_c = level_array_padded.shape\n",
    "    for x in range(0, image_w - 32, 16):\n",
    "        for y in range(0, image_h - 32, 16):\n",
    "            context_tile = level_array_padded[y : y + 48, x : x + 48, :]\n",
    "            \n",
    "            plt.imshow(context_tile)\n",
    "            plt.waitforbuttonpress()\n",
    "            \n",
    "            level_image_unrolled.append(context_tile)\n",
    "    return np.array(level_image_unrolled)\n",
    "\n",
    "def build_game_dataframe(current_game, game_image_dir, image_extension):\n",
    "    image_ids = set(\n",
    "        [\n",
    "            path.split(\"/\")[-1].split(\".\")[0]\n",
    "            for path in glob.glob(game_image_dir + \"/*\" + image_extension)\n",
    "        ]\n",
    "    )\n",
    "    ids = image_ids\n",
    "    # build a dataframe\n",
    "    image_paths = [game_image_dir + image_id + \".png\" for image_id in ids]\n",
    "    game_data = pd.DataFrame(columns=[\"image_path\"])\n",
    "    game_data[\"image_path\"] = image_paths\n",
    "    assert game_data.shape[0] == len(ids)\n",
    "    print(\"\\nAll Levels Loaded\")\n",
    "    print(\"\\nTotal Levels for game \", current_game, \" detected are \", len(ids))\n",
    "    return game_data, list(ids)\n",
    "\n",
    "def generate_unified_rep(current_game,loaded_game_data,game_image_dir, save_dir):\n",
    "    \n",
    "    ptr = 0\n",
    "\n",
    "    idx2embed_map = {}\n",
    "    idx2tile_map = {}\n",
    "\n",
    "    for idx in range(len(loaded_game_data)):\n",
    "\n",
    "        image_path = loaded_game_data.loc[idx][\"image_path\"]\n",
    "        level_id = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(\"\\nProcessing level\", level_id)\n",
    "        level_img, level_img_padded = get_image(image_path)\n",
    "        level_array = img_to_array(level_img)\n",
    "        level_array_padded = img_to_array(level_img_padded)\n",
    "\n",
    "        assert level_array.shape[0] % 16 == 0\n",
    "        assert level_array.shape[1] % 16 == 0\n",
    "        level_h = level_array.shape[0] / 16\n",
    "        level_w = level_array.shape[1] / 16\n",
    "        print(\"Height \", level_h, \"Width \", level_w)\n",
    "        level_image_expanded = level_image_unroll(level_array_padded)\n",
    "        print(\"Expanded level images \", level_image_expanded.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        mapped_text = np.zeros((level_image_expanded.shape[0], 13))\n",
    "        encoded_level = encoding_model.predict([level_image_expanded, mapped_text])\n",
    "        print(\"Encoding dimension\", encoded_level.shape)\n",
    "\n",
    "        for i in range(len(encoded_level)):\n",
    "            tile_embedding = encoded_level[i]\n",
    "            tile_sprite = level_image_expanded[i].reshape(48, 48, 3)[\n",
    "                16 : 16 + 16, 16 : 16 + 16, :\n",
    "            ]\n",
    "            idx2embed_map[ptr] = tile_embedding\n",
    "            idx2tile_map[ptr] = tile_sprite\n",
    "            ptr += 1\n",
    "\n",
    "    #     with open(save_dir + level_id + \".pickle\", \"wb\") as handle:\n",
    "    #         pickle.dump(encoded_level, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    #     print(\"Saved \", level_id, \" successfully!\")\n",
    "\n",
    "    # with open(save_dir + \"mappings/idx2embed.pickle\", \"wb\") as handle:\n",
    "    #     pickle.dump(idx2embed_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # print(\"Index to Embedding map saved successfully!\")\n",
    "\n",
    "    # with open(save_dir + \"mappings/idx2tile.pickle\", \"wb\") as handle:\n",
    "    #     pickle.dump(idx2tile_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # print(\"Index to Tile map saved successfully!\")\n",
    "    # print(\"Extracted unified representation for game \",current_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ac622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Levels Loaded\n",
      "\n",
      "Total Levels for game  bubble_bobble  detected are  95\n"
     ]
    }
   ],
   "source": [
    "current_game = \"bubble_bobble\"\n",
    "game_image_dir = \"../data/bubble_bobble/\"\n",
    "save_dir = \"../data/unified_rep/bubble_bobble/\"\n",
    "loaded_game_data, identifiers = build_game_dataframe(\n",
    "    current_game,\n",
    "    game_image_dir,\n",
    "    \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c36e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unified_rep(current_game,loaded_game_data,game_image_dir, save_dir)\n",
    "\n",
    "print(\"Saved Bubble Bobble Unified Representation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
