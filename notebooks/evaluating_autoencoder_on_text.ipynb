{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pregnant-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"import os\\nimport glob\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.utils import shuffle\\nimport json\\nimport pickle\\n\\nfrom keras.preprocessing import sequence, image\\nfrom keras.preprocessing.image import array_to_img, save_img, img_to_array\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nfrom keras.models import model_from_json\\nfrom keras.layers import (\\n    Flatten,\\n    Dense,\\n    Input,\\n    Activation,\\n    BatchNormalization,\\n    Conv2D,\\n    MaxPool2D,\\n    Dropout,\\n    UpSampling2D,\\n    Lambda,\\n)\\n\\nfrom keras.layers import ReLU, Reshape, Conv2DTranspose, Concatenate, Multiply\\nfrom keras.models import Model\\n\\nfrom keras.optimizers import Adam\\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\\nfrom keras import backend as K\\n\\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\\nfrom collections import Counter\\n\\n# split into train-test\\nfrom sklearn.model_selection import train_test_split\\nfrom notebook_utils import initialize_environment\\n\\ninitialize_environment()\\n\\nfrom utils.evaluation_metrics.multilabel.example_based import (\\n    hamming_loss,\\n    example_based_accuracy,\\n    example_based_precision,\\n    example_based_recall,\\n)\\n\\nfrom utils.evaluation_metrics.multilabel.label_based import (\\n    accuracy_macro,\\n    precision_macro,\\n    recall_macro,\\n    accuracy_micro,\\n    precision_micro,\\n    recall_micro,\\n)\\n\\nfrom utils.evaluation_metrics.multilabel.alpha_score import alpha_score\\nfrom utils.data_loading.load_data import get_tile_data\\n\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"import os\\nimport glob\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.utils import shuffle\\nimport json\\nimport pickle\\n\\nfrom keras.preprocessing import sequence, image\\nfrom keras.preprocessing.image import array_to_img, save_img, img_to_array\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nfrom keras.models import model_from_json\\nfrom keras.layers import (\\n    Flatten,\\n    Dense,\\n    Input,\\n    Activation,\\n    BatchNormalization,\\n    Conv2D,\\n    MaxPool2D,\\n    Dropout,\\n    UpSampling2D,\\n    Lambda,\\n)\\n\\nfrom keras.layers import ReLU, Reshape, Conv2DTranspose, Concatenate, Multiply\\nfrom keras.models import Model\\n\\nfrom keras.optimizers import Adam\\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\\nfrom keras import backend as K\\n\\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\\nfrom collections import Counter\\n\\n# split into train-test\\nfrom sklearn.model_selection import train_test_split\\nfrom notebook_utils import initialize_environment\\n\\ninitialize_environment()\\n\\nfrom utils.evaluation_metrics.multilabel.example_based import (\\n    hamming_loss,\\n    example_based_accuracy,\\n    example_based_precision,\\n    example_based_recall,\\n)\\n\\nfrom utils.evaluation_metrics.multilabel.label_based import (\\n    accuracy_macro,\\n    precision_macro,\\n    recall_macro,\\n    accuracy_micro,\\n    precision_micro,\\n    recall_micro,\\n)\\n\\nfrom utils.evaluation_metrics.multilabel.alpha_score import alpha_score\\nfrom utils.data_loading.load_data import get_tile_data\\n\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.preprocessing.image import array_to_img, save_img, img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Dropout,\n",
    "    UpSampling2D,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "from keras.layers import ReLU, Reshape, Conv2DTranspose, Concatenate, Multiply\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from notebook_utils import initialize_environment\n",
    "\n",
    "initialize_environment()\n",
    "\n",
    "from utils.evaluation_metrics.multilabel.example_based import (\n",
    "    hamming_loss,\n",
    "    example_based_accuracy,\n",
    "    example_based_precision,\n",
    "    example_based_recall,\n",
    ")\n",
    "\n",
    "from utils.evaluation_metrics.multilabel.label_based import (\n",
    "    accuracy_macro,\n",
    "    precision_macro,\n",
    "    recall_macro,\n",
    "    accuracy_micro,\n",
    "    precision_micro,\n",
    "    recall_micro,\n",
    ")\n",
    "\n",
    "from utils.evaluation_metrics.multilabel.alpha_score import alpha_score\n",
    "from utils.data_loading.load_data import get_tile_data\n",
    "\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alternate-butterfly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games detected in the parent folder ['lode_runner', 'kid_icarus', 'megaman', 'smb', 'loz']\n",
      "Current Game lode_runner\n",
      "Reading mappings\n",
      "Json File Loaded\n",
      "Reading Sprite Data From ../data/context_data/lode_runner\n",
      "Current Game kid_icarus\n",
      "Reading mappings\n",
      "Json File Loaded\n",
      "Reading Sprite Data From ../data/context_data/kid_icarus\n",
      "Current Game megaman\n",
      "Reading mappings\n",
      "Json File Loaded\n",
      "Reading Sprite Data From ../data/context_data/megaman\n",
      "Current Game smb\n",
      "Reading mappings\n",
      "Json File Loaded\n",
      "Reading Sprite Data From ../data/context_data/smb\n",
      "Current Game loz\n",
      "Reading mappings\n",
      "Json File Loaded\n",
      "Reading Sprite Data From ../data/context_data/loz\n",
      "\n",
      "The size of total data is (0, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-756340799f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nThe size of total data is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nThe size of the train data is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tile_representation-VeDour9V/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2176\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tile_representation-VeDour9V/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1861\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1862\u001b[0m         )\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"##loading train and testing data\\n\\ndata_directory = \\\"../data/context_data/\\\"\\njson_directory = \\\"../data/json_files_trimmed_features/\\\"\\ndata = get_tile_data(data_directory, json_directory)\\nprint(\\\"\\\\nThe size of total data is\\\", data.shape)\\ndata = shuffle(data)\\ntrain_data, test_data = train_test_split(data, test_size=0.10, random_state=42)\\n\\nprint(\\\"\\\\nThe size of the train data is \\\", train_data.shape)\\nprint(\\\"The size of the test data is \\\", test_data.shape)\";\n",
       "                var nbb_formatted_code = \"##loading train and testing data\\n\\ndata_directory = \\\"../data/context_data/\\\"\\njson_directory = \\\"../data/json_files_trimmed_features/\\\"\\ndata = get_tile_data(data_directory, json_directory)\\nprint(\\\"\\\\nThe size of total data is\\\", data.shape)\\ndata = shuffle(data)\\ntrain_data, test_data = train_test_split(data, test_size=0.10, random_state=42)\\n\\nprint(\\\"\\\\nThe size of the train data is \\\", train_data.shape)\\nprint(\\\"The size of the test data is \\\", test_data.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##loading train and testing data\n",
    "\n",
    "data_directory = \"../data/context_data/\"\n",
    "json_directory = \"../data/json_files_trimmed_features/\"\n",
    "data = get_tile_data(data_directory, json_directory)\n",
    "print(\"\\nThe size of total data is\", data.shape)\n",
    "data = shuffle(data)\n",
    "train_data, test_data = train_test_split(data, test_size=0.10, random_state=42)\n",
    "\n",
    "print(\"\\nThe size of the train data is \", train_data.shape)\n",
    "print(\"The size of the test data is \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the multilabel binarizer\n",
    "with open(\"../model/model_tokenizer.pickle\", \"rb\") as handle:\n",
    "    mlb = pickle.load(handle)\n",
    "print(\"Feature Dictionary Loaded\")\n",
    "total_features = len(mlb.classes_)\n",
    "print(\"The feature dictionary has size\", total_features)\n",
    "display(\"Features\", mlb.classes_)\n",
    "\n",
    "# load entire autoencoder architecture\n",
    "json_file = open(\"../model/autoencoder_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "ae_sep_output = model_from_json(loaded_model_json)\n",
    "ae_sep_output.load_weights(\"../model/autoencoder_model.h5\")\n",
    "print(\"Loaded Entire Autoencoder Model from the Disk\")\n",
    "\n",
    "# load the encoding architecture and weights\n",
    "json_file = open(\"../model/encoder_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "encoding_model = model_from_json(loaded_model_json)\n",
    "encoding_model.load_weights(\"../model/encoder_model.h5\")\n",
    "print(\"Loaded Encoder Model from the Disk\")\n",
    "\n",
    "# load the decoding architecture and weights\n",
    "json_file = open(\"../model/decoder_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "decoding_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "decoding_model.load_weights(\"../model/decoder_model.h5\")\n",
    "print(\"Loaded Decoder Model from the Disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Input Output Training Batches\n",
    "print(\"Building Training Batches\")\n",
    "\n",
    "\"\"\"Note : Add Generators\"\"\"\n",
    "train_image_batch = []\n",
    "for train_path in train_data[\"image_path\"]:\n",
    "    tile = image.load_img(train_path, target_size=(48, 48))\n",
    "    tile_sprite = image.img_to_array(tile)\n",
    "    train_image_batch.append(tile_sprite)\n",
    "train_image_batch = np.array(train_image_batch)\n",
    "train_text_batch = []\n",
    "for i in range(len(train_data[\"features\"])):\n",
    "    text_ = mlb.transform(train_data[\"features\"][i : i + 1])\n",
    "    train_text_batch.append(text_)\n",
    "train_text_batch = np.array(train_text_batch).reshape(\n",
    "    train_data.shape[0], total_features\n",
    ")\n",
    "\n",
    "output_image_batch = []\n",
    "for i in range(len(train_image_batch)):\n",
    "    current_image = train_image_batch[i]\n",
    "    current_image_centre = train_image_batch[i][16 : 16 + 16, 16 : 16 + 16, :]\n",
    "    output_image_batch.append(current_image_centre)\n",
    "output_image_batch = np.array(output_image_batch)\n",
    "output_text_batch = []\n",
    "for i in range(len(train_text_batch)):\n",
    "    current_text = train_text_batch[i]\n",
    "    output_text_batch.append(current_text)\n",
    "output_text_batch = np.array(output_text_batch)\n",
    "print(\"Training Data Ready\")\n",
    "print(\"Train Image batch shape\", train_image_batch.shape)\n",
    "print(\"Train Text batch shape\", train_text_batch.shape)\n",
    "print(\"Output Image batch shape\", output_image_batch.shape)\n",
    "print(\"Output Text batch shape\", output_text_batch.shape)\n",
    "\n",
    "\n",
    "# Build Input Output Test Batches\n",
    "print(\"Building Testing Batches\")\n",
    "\"\"\"Note : Add Generators\"\"\"\n",
    "test_image_batch = []\n",
    "for test_path in test_data[\"image_path\"]:\n",
    "    tile = image.load_img(test_path, target_size=(48, 48))\n",
    "    tile_sprite = image.img_to_array(tile)\n",
    "    test_image_batch.append(tile_sprite)\n",
    "test_image_batch = np.array(test_image_batch)\n",
    "test_text_batch = []\n",
    "for i in range(len(test_data[\"features\"])):\n",
    "    text_ = mlb.transform(test_data[\"features\"][i : i + 1])\n",
    "    test_text_batch.append(text_)\n",
    "test_text_batch = np.array(test_text_batch).reshape(test_data.shape[0], total_features)\n",
    "print(\"\\n\\nTesting Data Ready\")\n",
    "print(\"Train Image batch shape\", test_image_batch.shape)\n",
    "print(\"Train Text batch shape\", test_text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image, predicted_text = ae_sep_output.predict(\n",
    "    [test_image_batch, test_text_batch]\n",
    ")\n",
    "y_pred = [np.where(text > 0.5, 1, 0) for text in predicted_text]\n",
    "y_pred = np.array(y_pred)\n",
    "print(\"Predicted Y is Ready. Shape : \", y_pred.shape)\n",
    "\n",
    "y_true = test_text_batch\n",
    "y_true = np.array(y_true)\n",
    "print(\"True Y is Ready. Shape :\", y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_image = []\n",
    "for i in range(len(test_image_batch)):\n",
    "    current_image = test_image_batch[i]\n",
    "    current_image_centre = test_image_batch[i][16 : 16 + 16, 16 : 16 + 16, :]\n",
    "    true_image.append(current_image_centre)\n",
    "true_image = np.array(true_image)\n",
    "print(\"Predicted Array shape \", predicted_image.shape)\n",
    "print(\"True Array shape \", true_image.shape)\n",
    "\n",
    "mse_dist = []\n",
    "for idx in range(len(true_image)):\n",
    "    y_true_image = true_image[idx]\n",
    "    y_true_image = y_true_image.reshape(16, 16, 3)\n",
    "\n",
    "    y_pred_image = predicted_image[idx]\n",
    "    y_pred_image = y_pred_image.reshape(16, 16, 3)\n",
    "\n",
    "    mse_dist.append(np.mean(np.subtract(y_true_image, y_pred_image) ** 2))\n",
    "\n",
    "print(\"Mean MSE\", np.mean(mse_dist))\n",
    "print(\"Median MSE\", np.median(mse_dist))\n",
    "\n",
    "\n",
    "def valid_divide(num, den):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for idx in range(len(num)):\n",
    "        if num[idx] == den[idx] == 0:\n",
    "            continue\n",
    "        elif num[idx] != 0 and den[idx] != 0:\n",
    "            result[idx] = num[idx] / den[idx]\n",
    "            count += 1\n",
    "        elif num[idx] != 0 and den[idx] == 0 or num[idx] == 0 and den[idx] != 0:\n",
    "            count += 1\n",
    "            result[idx] = 0.0\n",
    "    return result, count\n",
    "\n",
    "\n",
    "print(\"\\nMacro Label Based Precision\", precision_macro(y_true, y_pred))\n",
    "print(\"Macro Label Based Recall\", recall_macro(y_true, y_pred))\n",
    "print(\"Macro Label Based Accuracy\", accuracy_macro(y_true, y_pred))\n",
    "\n",
    "print(\"\\nMicro Label Based Precision\", precision_micro(y_true, y_pred))\n",
    "print(\"Micro Label Based Recall\", recall_micro(y_true, y_pred))\n",
    "print(\"Micro Label Based Accuracy\", accuracy_micro(y_true, y_pred))\n",
    "\n",
    "print(\"\\nExample Based Precision\", example_based_precision(y_true, y_pred))\n",
    "print(\"Example Based Recall\", example_based_recall(y_true, y_pred))\n",
    "print(\"Example Based Accuracy\", example_based_accuracy(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-encoding",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tile_representation",
   "language": "python",
   "name": "tile_representation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
