{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.PTModel.Models import VGLCLSTMModel\n",
    "\n",
    "MODELNAME = \"VGCLLSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils.fastnumpyio as fnp\n",
    "\n",
    "def TrainModelFromFiles(batchPaths, epochs, batchSize, continueTraining=None, learningRate=0.001):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if continueTraining == None:\n",
    "        model = VGLCLSTMModel()\n",
    "    else:\n",
    "        model = continueTraining\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learningRate, eps=1e-7)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, eps=1e-7)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        losses.append([])\n",
    "\n",
    "        for t in range(0, len(batchPaths), 4):\n",
    "            print(t)\n",
    "            #print(batchPaths[t:t+4])\n",
    "\n",
    "            xTrain = fnp.load(batchPaths[t+1])\n",
    "            xTrainTargetIn = fnp.load(batchPaths[t+2])\n",
    "            yTrain = fnp.load(batchPaths[t+3])\n",
    "            columnRef = fnp.load(batchPaths[t])\n",
    "\n",
    "            for j in range(0, xTrain.shape[0], batchSize):\n",
    "\n",
    "                xTrain = xTrain.reshape(xTrain.shape[0], xTrain.shape[1], 1)\n",
    "                xTrainTargetIn = xTrainTargetIn.reshape(xTrainTargetIn.shape[0], xTrainTargetIn.shape[1], 1)\n",
    "                yTrain = yTrain.reshape(yTrain.shape[0], yTrain.shape[1], 1)\n",
    "                \n",
    "                xTrainTensor = torch.tensor(xTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "                xTrainTargetInTensor = torch.tensor(xTrainTargetIn[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                yTrainTensor = torch.tensor(yTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                columnRefTensor = torch.tensor(columnRef[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                # print(xTrainTensor.shape)\n",
    "                # print(xTrainTargetInTensor.shape)\n",
    "                # print(yTrainTensor.shape)\n",
    "                # print(columnRefTensor.shape)\n",
    "\n",
    "                #print(f\"xTrain size: {xTrainTensor.size()}\")\n",
    "\n",
    "                yPred = model(xTrainTensor, xTrainTargetInTensor, columnRefTensor)\n",
    "                \n",
    "                #print(f\"yPred size: {yPred.size()}\")\n",
    "                #print(f\"yTruth size: {yTrainTensor.size()}\")\n",
    "                loss = criterion(yPred, yTrainTensor)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses[i].append(loss.cpu().detach().item())\n",
    "\n",
    "            print(f\"Epoch {i} Batch {t}: loss {losses[i][-1]}\")\n",
    "\n",
    "        print(f\"Epoch {i}: loss {sum(losses[i])/len(losses[i])}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surfytom/miniconda3/envs/DissEnvFinal/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 97, 1])) that is different to the input size (torch.Size([32, 97, 9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0: loss 1.8350787162780762\n",
      "4\n",
      "Epoch 0 Batch 4: loss 10.182848930358887\n",
      "8\n",
      "Epoch 0 Batch 8: loss 8.909875869750977\n",
      "12\n",
      "Epoch 0 Batch 12: loss 8.01806640625\n",
      "16\n",
      "Epoch 0 Batch 16: loss 10.411195755004883\n",
      "20\n",
      "Epoch 0 Batch 20: loss 19.39141082763672\n",
      "24\n",
      "Epoch 0 Batch 24: loss 9.534256935119629\n",
      "28\n",
      "Epoch 0 Batch 28: loss 10.586026191711426\n",
      "32\n",
      "Epoch 0 Batch 32: loss 9.234169960021973\n",
      "36\n",
      "Epoch 0 Batch 36: loss 6.598904609680176\n",
      "40\n",
      "Epoch 0 Batch 40: loss 10.497014999389648\n",
      "44\n",
      "Epoch 0 Batch 44: loss 14.231578826904297\n",
      "48\n",
      "Epoch 0 Batch 48: loss 5.560441970825195\n",
      "52\n",
      "Epoch 0 Batch 52: loss 11.041385650634766\n",
      "56\n",
      "Epoch 0 Batch 56: loss 7.158257961273193\n",
      "60\n",
      "Epoch 0 Batch 60: loss 12.300336837768555\n",
      "64\n",
      "Epoch 0 Batch 64: loss 9.481467247009277\n",
      "68\n",
      "Epoch 0 Batch 68: loss 7.6795172691345215\n",
      "72\n",
      "Epoch 0 Batch 72: loss 10.083869934082031\n",
      "76\n",
      "Epoch 0 Batch 76: loss 8.255311965942383\n",
      "80\n",
      "Epoch 0 Batch 80: loss 5.891663551330566\n",
      "84\n",
      "Epoch 0 Batch 84: loss 2.5608203411102295\n",
      "88\n",
      "Epoch 0 Batch 88: loss 6.639000415802002\n",
      "92\n",
      "Epoch 0 Batch 92: loss 11.286718368530273\n",
      "96\n",
      "Epoch 0 Batch 96: loss 6.087522506713867\n",
      "Epoch 0: loss 9.90818381045994\n",
      "0\n",
      "Epoch 1 Batch 0: loss 1.8357173204421997\n",
      "4\n",
      "Epoch 1 Batch 4: loss 10.182978630065918\n",
      "8\n",
      "Epoch 1 Batch 8: loss 8.886902809143066\n",
      "12\n",
      "Epoch 1 Batch 12: loss 8.01128101348877\n",
      "16\n",
      "Epoch 1 Batch 16: loss 10.402268409729004\n",
      "20\n",
      "Epoch 1 Batch 20: loss 19.389490127563477\n",
      "24\n",
      "Epoch 1 Batch 24: loss 9.521175384521484\n",
      "28\n",
      "Epoch 1 Batch 28: loss 10.589707374572754\n",
      "32\n",
      "Epoch 1 Batch 32: loss 9.21939754486084\n",
      "36\n",
      "Epoch 1 Batch 36: loss 6.601315498352051\n",
      "40\n",
      "Epoch 1 Batch 40: loss 10.482026100158691\n",
      "44\n",
      "Epoch 1 Batch 44: loss 14.218084335327148\n",
      "48\n",
      "Epoch 1 Batch 48: loss 5.560507774353027\n",
      "52\n",
      "Epoch 1 Batch 52: loss 11.040863990783691\n",
      "56\n",
      "Epoch 1 Batch 56: loss 7.159048080444336\n",
      "60\n",
      "Epoch 1 Batch 60: loss 12.300387382507324\n",
      "64\n",
      "Epoch 1 Batch 64: loss 9.480399131774902\n",
      "68\n",
      "Epoch 1 Batch 68: loss 7.67970085144043\n",
      "72\n",
      "Epoch 1 Batch 72: loss 10.072375297546387\n",
      "76\n",
      "Epoch 1 Batch 76: loss 8.253338813781738\n",
      "80\n",
      "Epoch 1 Batch 80: loss 5.889828205108643\n",
      "84\n",
      "Epoch 1 Batch 84: loss 2.5599770545959473\n",
      "88\n",
      "Epoch 1 Batch 88: loss 6.638889789581299\n",
      "92\n",
      "Epoch 1 Batch 92: loss 11.28665828704834\n",
      "96\n",
      "Epoch 1 Batch 96: loss 6.086334705352783\n",
      "Epoch 1: loss 9.90586581397475\n",
      "0\n",
      "Epoch 2 Batch 0: loss 1.8350690603256226\n",
      "4\n",
      "Epoch 2 Batch 4: loss 10.18285846710205\n",
      "8\n",
      "Epoch 2 Batch 8: loss 8.89841079711914\n",
      "12\n",
      "Epoch 2 Batch 12: loss 8.009830474853516\n",
      "16\n",
      "Epoch 2 Batch 16: loss 10.407221794128418\n",
      "20\n",
      "Epoch 2 Batch 20: loss 19.391216278076172\n",
      "24\n",
      "Epoch 2 Batch 24: loss 9.532435417175293\n",
      "28\n",
      "Epoch 2 Batch 28: loss 10.584687232971191\n",
      "32\n",
      "Epoch 2 Batch 32: loss 9.22905158996582\n",
      "36\n",
      "Epoch 2 Batch 36: loss 6.595585823059082\n",
      "40\n",
      "Epoch 2 Batch 40: loss 10.47672176361084\n",
      "44\n",
      "Epoch 2 Batch 44: loss 14.221835136413574\n",
      "48\n",
      "Epoch 2 Batch 48: loss 5.560602188110352\n",
      "52\n",
      "Epoch 2 Batch 52: loss 11.033210754394531\n",
      "56\n",
      "Epoch 2 Batch 56: loss 7.159029006958008\n",
      "60\n",
      "Epoch 2 Batch 60: loss 12.300366401672363\n",
      "64\n",
      "Epoch 2 Batch 64: loss 9.48172664642334\n",
      "68\n",
      "Epoch 2 Batch 68: loss 7.679562568664551\n",
      "72\n",
      "Epoch 2 Batch 72: loss 10.074877738952637\n",
      "76\n",
      "Epoch 2 Batch 76: loss 8.249555587768555\n",
      "80\n",
      "Epoch 2 Batch 80: loss 5.893020153045654\n",
      "84\n",
      "Epoch 2 Batch 84: loss 2.5598461627960205\n",
      "88\n",
      "Epoch 2 Batch 88: loss 6.6388983726501465\n",
      "92\n",
      "Epoch 2 Batch 92: loss 11.28902530670166\n",
      "96\n",
      "Epoch 2 Batch 96: loss 6.0863037109375\n",
      "Epoch 2: loss 9.907207738809419\n",
      "0\n",
      "Epoch 3 Batch 0: loss 1.8351091146469116\n",
      "4\n",
      "Epoch 3 Batch 4: loss 10.18286418914795\n",
      "8\n",
      "Epoch 3 Batch 8: loss 8.91195297241211\n",
      "12\n",
      "Epoch 3 Batch 12: loss 8.009632110595703\n",
      "16\n",
      "Epoch 3 Batch 16: loss 10.40742301940918\n",
      "20\n",
      "Epoch 3 Batch 20: loss 19.39571762084961\n",
      "24\n",
      "Epoch 3 Batch 24: loss 9.543998718261719\n",
      "28\n",
      "Epoch 3 Batch 28: loss 10.58388614654541\n",
      "32\n",
      "Epoch 3 Batch 32: loss 9.252459526062012\n",
      "36\n",
      "Epoch 3 Batch 36: loss 6.615480899810791\n",
      "40\n",
      "Epoch 3 Batch 40: loss 10.510557174682617\n",
      "44\n",
      "Epoch 3 Batch 44: loss 14.231786727905273\n",
      "48\n",
      "Epoch 3 Batch 48: loss 5.560415267944336\n",
      "52\n",
      "Epoch 3 Batch 52: loss 11.04191780090332\n",
      "56\n",
      "Epoch 3 Batch 56: loss 7.158878326416016\n",
      "60\n",
      "Epoch 3 Batch 60: loss 12.300296783447266\n",
      "64\n",
      "Epoch 3 Batch 64: loss 9.482024192810059\n",
      "68\n",
      "Epoch 3 Batch 68: loss 7.679494857788086\n",
      "72\n",
      "Epoch 3 Batch 72: loss 10.079384803771973\n",
      "76\n",
      "Epoch 3 Batch 76: loss 8.250067710876465\n",
      "80\n",
      "Epoch 3 Batch 80: loss 5.887345314025879\n",
      "84\n",
      "Epoch 3 Batch 84: loss 2.5603110790252686\n",
      "88\n",
      "Epoch 3 Batch 88: loss 6.63900089263916\n",
      "92\n",
      "Epoch 3 Batch 92: loss 11.282435417175293\n",
      "96\n",
      "Epoch 3 Batch 96: loss 6.086333274841309\n",
      "Epoch 3: loss 9.908327138047468\n",
      "0\n",
      "Epoch 4 Batch 0: loss 1.8351175785064697\n",
      "4\n",
      "Epoch 4 Batch 4: loss 10.182910919189453\n",
      "8\n",
      "Epoch 4 Batch 8: loss 8.90027904510498\n",
      "12\n",
      "Epoch 4 Batch 12: loss 8.009654998779297\n",
      "16\n",
      "Epoch 4 Batch 16: loss 10.405252456665039\n",
      "20\n",
      "Epoch 4 Batch 20: loss 19.39033317565918\n",
      "24\n",
      "Epoch 4 Batch 24: loss 9.53042984008789\n",
      "28\n",
      "Epoch 4 Batch 28: loss 10.584442138671875\n",
      "32\n",
      "Epoch 4 Batch 32: loss 9.241859436035156\n",
      "36\n",
      "Epoch 4 Batch 36: loss 6.601170063018799\n",
      "40\n",
      "Epoch 4 Batch 40: loss 10.499484062194824\n",
      "44\n",
      "Epoch 4 Batch 44: loss 14.241888046264648\n",
      "48\n",
      "Epoch 4 Batch 48: loss 5.560419082641602\n",
      "52\n",
      "Epoch 4 Batch 52: loss 11.045920372009277\n",
      "56\n",
      "Epoch 4 Batch 56: loss 7.155674934387207\n",
      "60\n",
      "Epoch 4 Batch 60: loss 12.299665451049805\n",
      "64\n",
      "Epoch 4 Batch 64: loss 9.483084678649902\n",
      "68\n",
      "Epoch 4 Batch 68: loss 7.679490089416504\n",
      "72\n",
      "Epoch 4 Batch 72: loss 10.089706420898438\n",
      "76\n",
      "Epoch 4 Batch 76: loss 8.259719848632812\n",
      "80\n",
      "Epoch 4 Batch 80: loss 5.892931938171387\n",
      "84\n",
      "Epoch 4 Batch 84: loss 2.5598442554473877\n",
      "88\n",
      "Epoch 4 Batch 88: loss 6.638878345489502\n",
      "92\n",
      "Epoch 4 Batch 92: loss 11.289148330688477\n",
      "96\n",
      "Epoch 4 Batch 96: loss 6.08626651763916\n",
      "Epoch 4: loss 9.908804839075657\n",
      "0\n",
      "Epoch 5 Batch 0: loss 1.8350584506988525\n",
      "4\n",
      "Epoch 5 Batch 4: loss 10.182840347290039\n",
      "8\n",
      "Epoch 5 Batch 8: loss 8.919549942016602\n",
      "12\n",
      "Epoch 5 Batch 12: loss 8.009536743164062\n",
      "16\n",
      "Epoch 5 Batch 16: loss 10.409629821777344\n",
      "20\n",
      "Epoch 5 Batch 20: loss 19.39170265197754\n",
      "24\n",
      "Epoch 5 Batch 24: loss 9.547830581665039\n",
      "28\n",
      "Epoch 5 Batch 28: loss 10.587658882141113\n",
      "32\n",
      "Epoch 5 Batch 32: loss 9.241838455200195\n",
      "36\n",
      "Epoch 5 Batch 36: loss 6.595504283905029\n",
      "40\n",
      "Epoch 5 Batch 40: loss 10.485711097717285\n",
      "44\n",
      "Epoch 5 Batch 44: loss 14.219198226928711\n",
      "48\n",
      "Epoch 5 Batch 48: loss 5.560449123382568\n",
      "52\n",
      "Epoch 5 Batch 52: loss 11.04140567779541\n",
      "56\n",
      "Epoch 5 Batch 56: loss 7.156978607177734\n",
      "60\n",
      "Epoch 5 Batch 60: loss 12.300305366516113\n",
      "64\n",
      "Epoch 5 Batch 64: loss 9.4868803024292\n",
      "68\n",
      "Epoch 5 Batch 68: loss 7.692390441894531\n",
      "72\n",
      "Epoch 5 Batch 72: loss 10.097451210021973\n",
      "76\n",
      "Epoch 5 Batch 76: loss 8.248838424682617\n",
      "80\n",
      "Epoch 5 Batch 80: loss 5.889767646789551\n",
      "84\n",
      "Epoch 5 Batch 84: loss 2.560803174972534\n",
      "88\n",
      "Epoch 5 Batch 88: loss 6.638913154602051\n",
      "92\n",
      "Epoch 5 Batch 92: loss 11.287324905395508\n",
      "96\n",
      "Epoch 5 Batch 96: loss 6.086246013641357\n",
      "Epoch 5: loss 9.908686028823517\n",
      "0\n",
      "Epoch 6 Batch 0: loss 1.8351173400878906\n",
      "4\n",
      "Epoch 6 Batch 4: loss 10.182893753051758\n",
      "8\n",
      "Epoch 6 Batch 8: loss 8.909923553466797\n",
      "12\n",
      "Epoch 6 Batch 12: loss 8.009683609008789\n",
      "16\n",
      "Epoch 6 Batch 16: loss 10.41112995147705\n",
      "20\n",
      "Epoch 6 Batch 20: loss 19.393234252929688\n",
      "24\n",
      "Epoch 6 Batch 24: loss 9.530364036560059\n",
      "28\n",
      "Epoch 6 Batch 28: loss 10.584692001342773\n",
      "32\n",
      "Epoch 6 Batch 32: loss 9.23924732208252\n",
      "36\n",
      "Epoch 6 Batch 36: loss 6.598072528839111\n",
      "40\n",
      "Epoch 6 Batch 40: loss 10.485980033874512\n",
      "44\n",
      "Epoch 6 Batch 44: loss 14.228887557983398\n",
      "48\n",
      "Epoch 6 Batch 48: loss 5.5604424476623535\n",
      "52\n",
      "Epoch 6 Batch 52: loss 11.043964385986328\n",
      "56\n",
      "Epoch 6 Batch 56: loss 7.156313419342041\n",
      "60\n",
      "Epoch 6 Batch 60: loss 12.300291061401367\n",
      "64\n",
      "Epoch 6 Batch 64: loss 9.486846923828125\n",
      "68\n",
      "Epoch 6 Batch 68: loss 7.679488658905029\n",
      "72\n",
      "Epoch 6 Batch 72: loss 10.093555450439453\n",
      "76\n",
      "Epoch 6 Batch 76: loss 8.257950782775879\n",
      "80\n",
      "Epoch 6 Batch 80: loss 5.892268657684326\n",
      "84\n",
      "Epoch 6 Batch 84: loss 2.5600197315216064\n",
      "88\n",
      "Epoch 6 Batch 88: loss 6.638869285583496\n",
      "92\n",
      "Epoch 6 Batch 92: loss 11.288989067077637\n",
      "96\n",
      "Epoch 6 Batch 96: loss 6.086228847503662\n",
      "Epoch 6: loss 9.908629634631307\n",
      "0\n",
      "Epoch 7 Batch 0: loss 1.8350571393966675\n",
      "4\n",
      "Epoch 7 Batch 4: loss 10.18284797668457\n",
      "8\n",
      "Epoch 7 Batch 8: loss 8.904082298278809\n",
      "12\n",
      "Epoch 7 Batch 12: loss 8.020120620727539\n",
      "16\n",
      "Epoch 7 Batch 16: loss 10.407788276672363\n",
      "20\n",
      "Epoch 7 Batch 20: loss 19.3900089263916\n",
      "24\n",
      "Epoch 7 Batch 24: loss 9.530340194702148\n",
      "28\n",
      "Epoch 7 Batch 28: loss 10.585092544555664\n",
      "32\n",
      "Epoch 7 Batch 32: loss 9.241823196411133\n",
      "36\n",
      "Epoch 7 Batch 36: loss 6.609624862670898\n",
      "40\n",
      "Epoch 7 Batch 40: loss 10.487637519836426\n",
      "44\n",
      "Epoch 7 Batch 44: loss 14.22966194152832\n",
      "48\n",
      "Epoch 7 Batch 48: loss 5.5604376792907715\n",
      "52\n",
      "Epoch 7 Batch 52: loss 11.05214786529541\n",
      "56\n",
      "Epoch 7 Batch 56: loss 7.152426719665527\n",
      "60\n",
      "Epoch 7 Batch 60: loss 12.30026912689209\n",
      "64\n",
      "Epoch 7 Batch 64: loss 9.486796379089355\n",
      "68\n",
      "Epoch 7 Batch 68: loss 7.684622287750244\n",
      "72\n",
      "Epoch 7 Batch 72: loss 10.097419738769531\n",
      "76\n",
      "Epoch 7 Batch 76: loss 8.251456260681152\n",
      "80\n",
      "Epoch 7 Batch 80: loss 5.890349388122559\n",
      "84\n",
      "Epoch 7 Batch 84: loss 2.560307025909424\n",
      "88\n",
      "Epoch 7 Batch 88: loss 6.638863563537598\n",
      "92\n",
      "Epoch 7 Batch 92: loss 11.289018630981445\n",
      "96\n",
      "Epoch 7 Batch 96: loss 6.086216449737549\n",
      "Epoch 7: loss 9.90902519845126\n",
      "0\n",
      "Epoch 8 Batch 0: loss 1.835066556930542\n",
      "4\n",
      "Epoch 8 Batch 4: loss 10.182867050170898\n",
      "8\n",
      "Epoch 8 Batch 8: loss 8.892474174499512\n",
      "12\n",
      "Epoch 8 Batch 12: loss 8.018128395080566\n",
      "16\n",
      "Epoch 8 Batch 16: loss 10.40674114227295\n",
      "20\n",
      "Epoch 8 Batch 20: loss 19.39005470275879\n",
      "24\n",
      "Epoch 8 Batch 24: loss 9.53040599822998\n",
      "28\n",
      "Epoch 8 Batch 28: loss 10.58507251739502\n",
      "32\n",
      "Epoch 8 Batch 32: loss 9.252446174621582\n",
      "36\n",
      "Epoch 8 Batch 36: loss 6.611435890197754\n",
      "40\n",
      "Epoch 8 Batch 40: loss 10.497980117797852\n",
      "44\n",
      "Epoch 8 Batch 44: loss 14.229567527770996\n",
      "48\n",
      "Epoch 8 Batch 48: loss 5.560428619384766\n",
      "52\n",
      "Epoch 8 Batch 52: loss 11.043954849243164\n",
      "56\n",
      "Epoch 8 Batch 56: loss 7.1549973487854\n",
      "60\n",
      "Epoch 8 Batch 60: loss 12.300285339355469\n",
      "64\n",
      "Epoch 8 Batch 64: loss 9.485978126525879\n",
      "68\n",
      "Epoch 8 Batch 68: loss 7.68722677230835\n",
      "72\n",
      "Epoch 8 Batch 72: loss 10.101289749145508\n",
      "76\n",
      "Epoch 8 Batch 76: loss 8.24940299987793\n",
      "80\n",
      "Epoch 8 Batch 80: loss 5.889823913574219\n",
      "84\n",
      "Epoch 8 Batch 84: loss 2.56085205078125\n",
      "88\n",
      "Epoch 8 Batch 88: loss 6.640833854675293\n",
      "92\n",
      "Epoch 8 Batch 92: loss 11.277098655700684\n",
      "96\n",
      "Epoch 8 Batch 96: loss 6.098735809326172\n",
      "Epoch 8: loss 9.908910530784674\n",
      "0\n",
      "Epoch 9 Batch 0: loss 1.8355997800827026\n",
      "4\n",
      "Epoch 9 Batch 4: loss 10.183084487915039\n",
      "8\n",
      "Epoch 9 Batch 8: loss 8.887303352355957\n",
      "12\n",
      "Epoch 9 Batch 12: loss 8.057840347290039\n",
      "16\n",
      "Epoch 9 Batch 16: loss 10.402599334716797\n",
      "20\n",
      "Epoch 9 Batch 20: loss 19.39299201965332\n",
      "24\n",
      "Epoch 9 Batch 24: loss 9.520858764648438\n",
      "28\n",
      "Epoch 9 Batch 28: loss 10.590619087219238\n",
      "32\n",
      "Epoch 9 Batch 32: loss 9.221147537231445\n",
      "36\n",
      "Epoch 9 Batch 36: loss 6.603724956512451\n",
      "40\n",
      "Epoch 9 Batch 40: loss 10.477664947509766\n",
      "44\n",
      "Epoch 9 Batch 44: loss 14.2213773727417\n",
      "48\n",
      "Epoch 9 Batch 48: loss 5.560610294342041\n",
      "52\n",
      "Epoch 9 Batch 52: loss 11.036833763122559\n",
      "56\n",
      "Epoch 9 Batch 56: loss 7.159015655517578\n",
      "60\n",
      "Epoch 9 Batch 60: loss 12.300436973571777\n",
      "64\n",
      "Epoch 9 Batch 64: loss 9.476405143737793\n",
      "68\n",
      "Epoch 9 Batch 68: loss 7.679582595825195\n",
      "72\n",
      "Epoch 9 Batch 72: loss 10.091582298278809\n",
      "76\n",
      "Epoch 9 Batch 76: loss 8.257862091064453\n",
      "80\n",
      "Epoch 9 Batch 80: loss 5.884425163269043\n",
      "84\n",
      "Epoch 9 Batch 84: loss 2.560105800628662\n",
      "88\n",
      "Epoch 9 Batch 88: loss 6.6390252113342285\n",
      "92\n",
      "Epoch 9 Batch 92: loss 11.274417877197266\n",
      "96\n",
      "Epoch 9 Batch 96: loss 6.086456775665283\n",
      "Epoch 9: loss 9.905051944381313\n",
      "0\n",
      "Epoch 10 Batch 0: loss 1.8353703022003174\n",
      "4\n",
      "Epoch 10 Batch 4: loss 10.183131217956543\n",
      "8\n",
      "Epoch 10 Batch 8: loss 8.887029647827148\n",
      "12\n",
      "Epoch 10 Batch 12: loss 8.044614791870117\n",
      "16\n",
      "Epoch 10 Batch 16: loss 10.40217399597168\n",
      "20\n",
      "Epoch 10 Batch 20: loss 19.394264221191406\n",
      "24\n",
      "Epoch 10 Batch 24: loss 9.521712303161621\n",
      "28\n",
      "Epoch 10 Batch 28: loss 10.598739624023438\n",
      "32\n",
      "Epoch 10 Batch 32: loss 9.232196807861328\n",
      "36\n",
      "Epoch 10 Batch 36: loss 6.605149269104004\n",
      "40\n",
      "Epoch 10 Batch 40: loss 10.50317668914795\n",
      "44\n",
      "Epoch 10 Batch 44: loss 14.231861114501953\n",
      "48\n",
      "Epoch 10 Batch 48: loss 5.5607171058654785\n",
      "52\n",
      "Epoch 10 Batch 52: loss 11.031673431396484\n",
      "56\n",
      "Epoch 10 Batch 56: loss 7.163724422454834\n",
      "60\n",
      "Epoch 10 Batch 60: loss 12.300796508789062\n",
      "64\n",
      "Epoch 10 Batch 64: loss 9.470026016235352\n",
      "68\n",
      "Epoch 10 Batch 68: loss 7.679698467254639\n",
      "72\n",
      "Epoch 10 Batch 72: loss 10.09814453125\n",
      "76\n",
      "Epoch 10 Batch 76: loss 8.257942199707031\n",
      "80\n",
      "Epoch 10 Batch 80: loss 5.884300708770752\n",
      "84\n",
      "Epoch 10 Batch 84: loss 2.5598795413970947\n",
      "88\n",
      "Epoch 10 Batch 88: loss 6.638993740081787\n",
      "92\n",
      "Epoch 10 Batch 92: loss 11.279211044311523\n",
      "96\n",
      "Epoch 10 Batch 96: loss 6.08626651763916\n",
      "Epoch 10: loss 9.903594559786612\n",
      "0\n",
      "Epoch 11 Batch 0: loss 1.8351399898529053\n",
      "4\n",
      "Epoch 11 Batch 4: loss 10.182892799377441\n",
      "8\n",
      "Epoch 11 Batch 8: loss 8.887226104736328\n",
      "12\n",
      "Epoch 11 Batch 12: loss 8.00993537902832\n",
      "16\n",
      "Epoch 11 Batch 16: loss 10.411184310913086\n",
      "20\n",
      "Epoch 11 Batch 20: loss 19.39042091369629\n",
      "24\n",
      "Epoch 11 Batch 24: loss 9.520763397216797\n",
      "28\n",
      "Epoch 11 Batch 28: loss 10.5820951461792\n",
      "32\n",
      "Epoch 11 Batch 32: loss 9.227818489074707\n",
      "36\n",
      "Epoch 11 Batch 36: loss 6.593560218811035\n",
      "40\n",
      "Epoch 11 Batch 40: loss 10.469230651855469\n",
      "44\n",
      "Epoch 11 Batch 44: loss 14.219276428222656\n",
      "48\n",
      "Epoch 11 Batch 48: loss 5.560476779937744\n",
      "52\n",
      "Epoch 11 Batch 52: loss 11.043933868408203\n",
      "56\n",
      "Epoch 11 Batch 56: loss 7.15768575668335\n",
      "60\n",
      "Epoch 11 Batch 60: loss 12.300297737121582\n",
      "64\n",
      "Epoch 11 Batch 64: loss 9.483471870422363\n",
      "68\n",
      "Epoch 11 Batch 68: loss 7.679558753967285\n",
      "72\n",
      "Epoch 11 Batch 72: loss 10.0767183303833\n",
      "76\n",
      "Epoch 11 Batch 76: loss 8.248444557189941\n",
      "80\n",
      "Epoch 11 Batch 80: loss 5.89035701751709\n",
      "84\n",
      "Epoch 11 Batch 84: loss 2.559696674346924\n",
      "88\n",
      "Epoch 11 Batch 88: loss 6.63886833190918\n",
      "92\n",
      "Epoch 11 Batch 92: loss 11.288995742797852\n",
      "96\n",
      "Epoch 11 Batch 96: loss 6.0862226486206055\n",
      "Epoch 11: loss 9.906962941362147\n",
      "0\n",
      "Epoch 12 Batch 0: loss 1.8350671529769897\n",
      "4\n",
      "Epoch 12 Batch 4: loss 10.182847023010254\n",
      "8\n",
      "Epoch 12 Batch 8: loss 8.898331642150879\n",
      "12\n",
      "Epoch 12 Batch 12: loss 8.056804656982422\n",
      "16\n",
      "Epoch 12 Batch 16: loss 10.413348197937012\n",
      "20\n",
      "Epoch 12 Batch 20: loss 19.395172119140625\n",
      "24\n",
      "Epoch 12 Batch 24: loss 9.520869255065918\n",
      "28\n",
      "Epoch 12 Batch 28: loss 10.587013244628906\n",
      "32\n",
      "Epoch 12 Batch 32: loss 9.230337142944336\n",
      "36\n",
      "Epoch 12 Batch 36: loss 6.594249248504639\n",
      "40\n",
      "Epoch 12 Batch 40: loss 10.484757423400879\n",
      "44\n",
      "Epoch 12 Batch 44: loss 14.22422981262207\n",
      "48\n",
      "Epoch 12 Batch 48: loss 5.560445785522461\n",
      "52\n",
      "Epoch 12 Batch 52: loss 11.044525146484375\n",
      "56\n",
      "Epoch 12 Batch 56: loss 7.15889310836792\n",
      "60\n",
      "Epoch 12 Batch 60: loss 12.30029296875\n",
      "64\n",
      "Epoch 12 Batch 64: loss 9.47874641418457\n",
      "68\n",
      "Epoch 12 Batch 68: loss 7.679527282714844\n",
      "72\n",
      "Epoch 12 Batch 72: loss 10.080018997192383\n",
      "76\n",
      "Epoch 12 Batch 76: loss 8.255219459533691\n",
      "80\n",
      "Epoch 12 Batch 80: loss 5.889730453491211\n",
      "84\n",
      "Epoch 12 Batch 84: loss 2.560107946395874\n",
      "88\n",
      "Epoch 12 Batch 88: loss 6.638877868652344\n",
      "92\n",
      "Epoch 12 Batch 92: loss 11.281700134277344\n",
      "96\n",
      "Epoch 12 Batch 96: loss 6.086219787597656\n",
      "Epoch 12: loss 9.907326242589114\n",
      "0\n",
      "Epoch 13 Batch 0: loss 1.835083246231079\n",
      "4\n",
      "Epoch 13 Batch 4: loss 10.182884216308594\n",
      "8\n",
      "Epoch 13 Batch 8: loss 8.88676929473877\n",
      "12\n",
      "Epoch 13 Batch 12: loss 8.009772300720215\n",
      "16\n",
      "Epoch 13 Batch 16: loss 10.407301902770996\n",
      "20\n",
      "Epoch 13 Batch 20: loss 19.389999389648438\n",
      "24\n",
      "Epoch 13 Batch 24: loss 9.521126747131348\n",
      "28\n",
      "Epoch 13 Batch 28: loss 10.581883430480957\n",
      "32\n",
      "Epoch 13 Batch 32: loss 9.231603622436523\n",
      "36\n",
      "Epoch 13 Batch 36: loss 6.593533992767334\n",
      "40\n",
      "Epoch 13 Batch 40: loss 10.470746994018555\n",
      "44\n",
      "Epoch 13 Batch 44: loss 14.2184419631958\n",
      "48\n",
      "Epoch 13 Batch 48: loss 5.560445308685303\n",
      "52\n",
      "Epoch 13 Batch 52: loss 11.033693313598633\n",
      "56\n",
      "Epoch 13 Batch 56: loss 7.158895969390869\n",
      "60\n",
      "Epoch 13 Batch 60: loss 12.314814567565918\n",
      "64\n",
      "Epoch 13 Batch 64: loss 9.474455833435059\n",
      "68\n",
      "Epoch 13 Batch 68: loss 7.6795334815979\n",
      "72\n",
      "Epoch 13 Batch 72: loss 10.091550827026367\n",
      "76\n",
      "Epoch 13 Batch 76: loss 8.257830619812012\n",
      "80\n",
      "Epoch 13 Batch 80: loss 5.886115550994873\n",
      "84\n",
      "Epoch 13 Batch 84: loss 2.5598440170288086\n",
      "88\n",
      "Epoch 13 Batch 88: loss 6.6389312744140625\n",
      "92\n",
      "Epoch 13 Batch 92: loss 11.288530349731445\n",
      "96\n",
      "Epoch 13 Batch 96: loss 6.086247444152832\n",
      "Epoch 13: loss 9.906454912612313\n",
      "0\n",
      "Epoch 14 Batch 0: loss 1.8350661993026733\n",
      "4\n",
      "Epoch 14 Batch 4: loss 10.182851791381836\n",
      "8\n",
      "Epoch 14 Batch 8: loss 8.892752647399902\n",
      "12\n",
      "Epoch 14 Batch 12: loss 8.009716987609863\n",
      "16\n",
      "Epoch 14 Batch 16: loss 10.409222602844238\n",
      "20\n",
      "Epoch 14 Batch 20: loss 19.392263412475586\n",
      "24\n",
      "Epoch 14 Batch 24: loss 9.522971153259277\n",
      "28\n",
      "Epoch 14 Batch 28: loss 10.583791732788086\n",
      "32\n",
      "Epoch 14 Batch 32: loss 9.243134498596191\n",
      "36\n",
      "Epoch 14 Batch 36: loss 6.593708515167236\n",
      "40\n",
      "Epoch 14 Batch 40: loss 10.486584663391113\n",
      "44\n",
      "Epoch 14 Batch 44: loss 14.225252151489258\n",
      "48\n",
      "Epoch 14 Batch 48: loss 5.560526371002197\n",
      "52\n",
      "Epoch 14 Batch 52: loss 11.041121482849121\n",
      "56\n",
      "Epoch 14 Batch 56: loss 7.158870697021484\n",
      "60\n",
      "Epoch 14 Batch 60: loss 12.305280685424805\n",
      "64\n",
      "Epoch 14 Batch 64: loss 9.476755142211914\n",
      "68\n",
      "Epoch 14 Batch 68: loss 7.679513931274414\n",
      "72\n",
      "Epoch 14 Batch 72: loss 10.072379112243652\n",
      "76\n",
      "Epoch 14 Batch 76: loss 8.249506950378418\n",
      "80\n",
      "Epoch 14 Batch 80: loss 5.89235782623291\n",
      "84\n",
      "Epoch 14 Batch 84: loss 2.5597007274627686\n",
      "88\n",
      "Epoch 14 Batch 88: loss 6.63889741897583\n",
      "92\n",
      "Epoch 14 Batch 92: loss 11.289013862609863\n",
      "96\n",
      "Epoch 14 Batch 96: loss 6.086231231689453\n",
      "Epoch 14: loss 9.907834085288801\n",
      "0\n",
      "Epoch 15 Batch 0: loss 1.8351058959960938\n",
      "4\n",
      "Epoch 15 Batch 4: loss 10.182685852050781\n",
      "8\n",
      "Epoch 15 Batch 8: loss 8.892678260803223\n",
      "12\n",
      "Epoch 15 Batch 12: loss 8.010498046875\n",
      "16\n",
      "Epoch 15 Batch 16: loss 10.408575057983398\n",
      "20\n",
      "Epoch 15 Batch 20: loss 19.390518188476562\n",
      "24\n",
      "Epoch 15 Batch 24: loss 9.520712852478027\n",
      "28\n",
      "Epoch 15 Batch 28: loss 10.582145690917969\n",
      "32\n",
      "Epoch 15 Batch 32: loss 9.228994369506836\n",
      "36\n",
      "Epoch 15 Batch 36: loss 6.596778392791748\n",
      "40\n",
      "Epoch 15 Batch 40: loss 10.477265357971191\n",
      "44\n",
      "Epoch 15 Batch 44: loss 14.217996597290039\n",
      "48\n",
      "Epoch 15 Batch 48: loss 5.560464382171631\n",
      "52\n",
      "Epoch 15 Batch 52: loss 11.040060997009277\n",
      "56\n",
      "Epoch 15 Batch 56: loss 7.158895015716553\n",
      "60\n",
      "Epoch 15 Batch 60: loss 12.316581726074219\n",
      "64\n",
      "Epoch 15 Batch 64: loss 9.4718599319458\n",
      "68\n",
      "Epoch 15 Batch 68: loss 7.679577827453613\n",
      "72\n",
      "Epoch 15 Batch 72: loss 10.085185050964355\n",
      "76\n",
      "Epoch 15 Batch 76: loss 8.257917404174805\n",
      "80\n",
      "Epoch 15 Batch 80: loss 5.889362335205078\n",
      "84\n",
      "Epoch 15 Batch 84: loss 2.5598132610321045\n",
      "88\n",
      "Epoch 15 Batch 88: loss 6.638924598693848\n",
      "92\n",
      "Epoch 15 Batch 92: loss 11.279800415039062\n",
      "96\n",
      "Epoch 15 Batch 96: loss 6.0863356590271\n",
      "Epoch 15: loss 9.906408388991105\n",
      "0\n",
      "Epoch 16 Batch 0: loss 1.835089087486267\n",
      "4\n",
      "Epoch 16 Batch 4: loss 10.182881355285645\n",
      "8\n",
      "Epoch 16 Batch 8: loss 8.886740684509277\n",
      "12\n",
      "Epoch 16 Batch 12: loss 8.013625144958496\n",
      "16\n",
      "Epoch 16 Batch 16: loss 10.402271270751953\n",
      "20\n",
      "Epoch 16 Batch 20: loss 19.38970375061035\n",
      "24\n",
      "Epoch 16 Batch 24: loss 9.520730018615723\n",
      "28\n",
      "Epoch 16 Batch 28: loss 10.579339981079102\n",
      "32\n",
      "Epoch 16 Batch 32: loss 9.235492706298828\n",
      "36\n",
      "Epoch 16 Batch 36: loss 6.59033727645874\n",
      "40\n",
      "Epoch 16 Batch 40: loss 10.48654556274414\n",
      "44\n",
      "Epoch 16 Batch 44: loss 14.22899055480957\n",
      "48\n",
      "Epoch 16 Batch 48: loss 5.560453414916992\n",
      "52\n",
      "Epoch 16 Batch 52: loss 11.045808792114258\n",
      "56\n",
      "Epoch 16 Batch 56: loss 7.158892631530762\n",
      "60\n",
      "Epoch 16 Batch 60: loss 12.300272941589355\n",
      "64\n",
      "Epoch 16 Batch 64: loss 9.48277759552002\n",
      "68\n",
      "Epoch 16 Batch 68: loss 7.679522514343262\n",
      "72\n",
      "Epoch 16 Batch 72: loss 10.078766822814941\n",
      "76\n",
      "Epoch 16 Batch 76: loss 8.248420715332031\n",
      "80\n",
      "Epoch 16 Batch 80: loss 5.890329360961914\n",
      "84\n",
      "Epoch 16 Batch 84: loss 2.5600850582122803\n",
      "88\n",
      "Epoch 16 Batch 88: loss 6.638863563537598\n",
      "92\n",
      "Epoch 16 Batch 92: loss 11.285368919372559\n",
      "96\n",
      "Epoch 16 Batch 96: loss 6.086203098297119\n",
      "Epoch 16: loss 9.906613522245173\n",
      "0\n",
      "Epoch 17 Batch 0: loss 1.835068941116333\n",
      "4\n",
      "Epoch 17 Batch 4: loss 10.182936668395996\n",
      "8\n",
      "Epoch 17 Batch 8: loss 8.898674011230469\n",
      "12\n",
      "Epoch 17 Batch 12: loss 8.00967788696289\n",
      "16\n",
      "Epoch 17 Batch 16: loss 10.41048812866211\n",
      "20\n",
      "Epoch 17 Batch 20: loss 19.393098831176758\n",
      "24\n",
      "Epoch 17 Batch 24: loss 9.528839111328125\n",
      "28\n",
      "Epoch 17 Batch 28: loss 10.582575798034668\n",
      "32\n",
      "Epoch 17 Batch 32: loss 9.235377311706543\n",
      "36\n",
      "Epoch 17 Batch 36: loss 6.591638565063477\n",
      "40\n",
      "Epoch 17 Batch 40: loss 10.476542472839355\n",
      "44\n",
      "Epoch 17 Batch 44: loss 14.226408004760742\n",
      "48\n",
      "Epoch 17 Batch 48: loss 5.560418128967285\n",
      "52\n",
      "Epoch 17 Batch 52: loss 11.042248725891113\n",
      "56\n",
      "Epoch 17 Batch 56: loss 7.158865928649902\n",
      "60\n",
      "Epoch 17 Batch 60: loss 12.301420211791992\n",
      "64\n",
      "Epoch 17 Batch 64: loss 9.478816032409668\n",
      "68\n",
      "Epoch 17 Batch 68: loss 7.6794843673706055\n",
      "72\n",
      "Epoch 17 Batch 72: loss 10.085103034973145\n",
      "76\n",
      "Epoch 17 Batch 76: loss 8.251357078552246\n",
      "80\n",
      "Epoch 17 Batch 80: loss 5.889285564422607\n",
      "84\n",
      "Epoch 17 Batch 84: loss 2.5598702430725098\n",
      "88\n",
      "Epoch 17 Batch 88: loss 6.638900279998779\n",
      "92\n",
      "Epoch 17 Batch 92: loss 11.285991668701172\n",
      "96\n",
      "Epoch 17 Batch 96: loss 6.086259365081787\n",
      "Epoch 17: loss 9.9074248780284\n",
      "0\n",
      "Epoch 18 Batch 0: loss 1.8351212739944458\n",
      "4\n",
      "Epoch 18 Batch 4: loss 10.182847023010254\n",
      "8\n",
      "Epoch 18 Batch 8: loss 8.909965515136719\n",
      "12\n",
      "Epoch 18 Batch 12: loss 8.018876075744629\n",
      "16\n",
      "Epoch 18 Batch 16: loss 10.40341567993164\n",
      "20\n",
      "Epoch 18 Batch 20: loss 19.394996643066406\n",
      "24\n",
      "Epoch 18 Batch 24: loss 9.521012306213379\n",
      "28\n",
      "Epoch 18 Batch 28: loss 10.59970474243164\n",
      "32\n",
      "Epoch 18 Batch 32: loss 9.232895851135254\n",
      "36\n",
      "Epoch 18 Batch 36: loss 6.627011775970459\n",
      "40\n",
      "Epoch 18 Batch 40: loss 10.493317604064941\n",
      "44\n",
      "Epoch 18 Batch 44: loss 14.236448287963867\n",
      "48\n",
      "Epoch 18 Batch 48: loss 5.560609817504883\n",
      "52\n",
      "Epoch 18 Batch 52: loss 11.031665802001953\n",
      "56\n",
      "Epoch 18 Batch 56: loss 7.165665626525879\n",
      "60\n",
      "Epoch 18 Batch 60: loss 12.300996780395508\n",
      "64\n",
      "Epoch 18 Batch 64: loss 9.472983360290527\n",
      "68\n",
      "Epoch 18 Batch 68: loss 7.679710388183594\n",
      "72\n",
      "Epoch 18 Batch 72: loss 10.1138334274292\n",
      "76\n",
      "Epoch 18 Batch 76: loss 8.25922679901123\n",
      "80\n",
      "Epoch 18 Batch 80: loss 5.884088039398193\n",
      "84\n",
      "Epoch 18 Batch 84: loss 2.5606040954589844\n",
      "88\n",
      "Epoch 18 Batch 88: loss 6.639261245727539\n",
      "92\n",
      "Epoch 18 Batch 92: loss 11.269800186157227\n",
      "96\n",
      "Epoch 18 Batch 96: loss 6.087032318115234\n",
      "Epoch 18: loss 9.905144870950465\n",
      "0\n",
      "Epoch 19 Batch 0: loss 1.8356143236160278\n",
      "4\n",
      "Epoch 19 Batch 4: loss 10.183252334594727\n",
      "8\n",
      "Epoch 19 Batch 8: loss 8.888254165649414\n",
      "12\n",
      "Epoch 19 Batch 12: loss 8.044183731079102\n",
      "16\n",
      "Epoch 19 Batch 16: loss 10.416420936584473\n",
      "20\n",
      "Epoch 19 Batch 20: loss 19.393253326416016\n",
      "24\n",
      "Epoch 19 Batch 24: loss 9.522052764892578\n",
      "28\n",
      "Epoch 19 Batch 28: loss 10.59865951538086\n",
      "32\n",
      "Epoch 19 Batch 32: loss 9.244592666625977\n",
      "36\n",
      "Epoch 19 Batch 36: loss 6.643115997314453\n",
      "40\n",
      "Epoch 19 Batch 40: loss 10.505254745483398\n",
      "44\n",
      "Epoch 19 Batch 44: loss 14.238611221313477\n",
      "48\n",
      "Epoch 19 Batch 48: loss 5.5608367919921875\n",
      "52\n",
      "Epoch 19 Batch 52: loss 11.031630516052246\n",
      "56\n",
      "Epoch 19 Batch 56: loss 7.178990840911865\n",
      "60\n",
      "Epoch 19 Batch 60: loss 12.324821472167969\n",
      "64\n",
      "Epoch 19 Batch 64: loss 9.47383975982666\n",
      "68\n",
      "Epoch 19 Batch 68: loss 7.680028915405273\n",
      "72\n",
      "Epoch 19 Batch 72: loss 10.11413860321045\n",
      "76\n",
      "Epoch 19 Batch 76: loss 8.26361083984375\n",
      "80\n",
      "Epoch 19 Batch 80: loss 5.895820617675781\n",
      "84\n",
      "Epoch 19 Batch 84: loss 2.5619056224823\n",
      "88\n",
      "Epoch 19 Batch 88: loss 6.639294147491455\n",
      "92\n",
      "Epoch 19 Batch 92: loss 11.285223960876465\n",
      "96\n",
      "Epoch 19 Batch 96: loss 6.096100330352783\n",
      "Epoch 19: loss 9.90551952336964\n"
     ]
    }
   ],
   "source": [
    "model = TrainModelFromFiles(sorted(glob.glob(f\"Models/{MODELNAME}/LRLSTMData/*\")), 20, 32, learningRate=0.0001, continueTraining=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"Models/{MODELNAME}/LodeRunnerLSTM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histLSTM.weight_ih_l0   :   512\n",
      "histLSTM.weight_hh_l0   : 65536\n",
      "histLSTM.bias_ih_l0     :   512\n",
      "histLSTM.bias_hh_l0     :   512\n",
      "colLSTM.weight_ih_l0    : 131072\n",
      "colLSTM.weight_hh_l0    : 65536\n",
      "colLSTM.bias_ih_l0      :   512\n",
      "colLSTM.bias_hh_l0      :   512\n",
      "textLSTM.weight_ih_l0   :   512\n",
      "textLSTM.weight_hh_l0   : 65536\n",
      "textLSTM.bias_ih_l0     :   512\n",
      "textLSTM.bias_hh_l0     :   512\n",
      "outputLayer.weight      :  1152\n",
      "outputLayer.bias        :     9\n",
      "Total Params: 332937\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:<24}: {param.numel():5}\")\n",
    "    total += param.numel()\n",
    "\n",
    "print(f\"Total Params: {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
