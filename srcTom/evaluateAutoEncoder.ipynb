{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import Utils.EvaluationMetrics.alphaScore as AS\n",
    "import Utils.EvaluationMetrics.exampleBased as EB\n",
    "import Utils.EvaluationMetrics.labelBased as LB\n",
    "\n",
    "import Utils.PTModel.Inference as Inference\n",
    "import Utils.Data.DataLoading as DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Models/TestModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Weight Array: tensor([4.8869, 3.9455, 1.9198, 2.5765, 2.4511, 0.0000, 2.0747, 6.3910, 8.0251,\n",
      "        0.0000, 0.0000, 1.5054, 7.5019])\n"
     ]
    }
   ],
   "source": [
    "dataset = DataLoading.LoadTrainTestData(\"/home/surfytom/Projects/Dissertation/Repos/TileEmbeddingDissertation/data/tomData/unshuffled3x3tiles.csv\")\n",
    "dataset = pd.concat([dataset[\"trainData\"], dataset[\"testData\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surfytom/Projects/Dissertation/Repos/TileEmbeddingDissertation/srcTom/Utils/PTModel/Inference.py:86: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  xTextbatch = torch.tensor(data[\"encodedAffordances\"].tolist(), dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "yPredImage, yPredText = Inference.ModelInference(model, dataset)\n",
    "yTruthText = np.array(dataset[\"encodedAffordances\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853833841228799"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AS.alpha_score(yTruthText, yPredText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14153353649151965\n",
      "0.9985229443339836\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(EB.example_based_accuracy(yTruthText, yPredText))\n",
    "print(EB.example_based_precision(yTruthText, yPredText))\n",
    "print(EB.example_based_recall(yTruthText, yPredText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro\n",
      "0.09629498\n",
      "0.17548285\n",
      "0.1758645\n",
      "\n",
      "Macro\n",
      "0.050115411098186784\n",
      "0.05121747805522038\n",
      "0.10862692502828744\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro\")\n",
    "print(LB.accuracy_micro(yTruthText, yPredText))\n",
    "print(LB.precision_micro(yTruthText, yPredText))\n",
    "print(LB.recall_micro(yTruthText, yPredText))\n",
    "\n",
    "print(\"\\nMacro\")\n",
    "print(LB.accuracy_macro(yTruthText, yPredText))\n",
    "print(LB.precision_macro(yTruthText, yPredText))\n",
    "print(LB.recall_macro(yTruthText, yPredText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_micro(y_true,y_pred):\n",
    "\n",
    "    precision_num = 0\n",
    "    for i in range(13):\n",
    "        array = [0] * 13\n",
    "        array[i] = 1\n",
    "        # Not Correct cuz its done on ytruthtext which is not a predictor please fix to be actual recall of predicted arrays\n",
    "        precision_num += sum(np.sum(np.logical_not(np.logical_xor(yTruthText, array)), axis=1) == 13)\n",
    "\n",
    "    precision_den=np.sum(y_true)\n",
    "    print(precision_den)\n",
    "\n",
    "    p_n=np.array(precision_num).astype(np.float32)\n",
    "    p_d=np.array(precision_den).astype(np.float32)\n",
    "\n",
    "    micro_recall=(p_n/p_d)\n",
    "    \n",
    "    return micro_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0878371"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_micro(yTruthText, yPredText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
