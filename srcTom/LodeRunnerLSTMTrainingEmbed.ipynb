{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Utils.PTModel.Models import LSTMModel\n",
    "\n",
    "MODELNAME = \"TestVAEwATT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameName = \"LodeRunner\"\n",
    "rowLength = 32\n",
    "numOfRows = 22\n",
    "\n",
    "lrEmbeddingPath = f\"Models/{MODELNAME}/LevelUnifiedRep/{gameName}\"\n",
    "lrEmbeddingPaths = sorted(glob.glob(f\"{lrEmbeddingPath}/level*.npy\"))\n",
    "\n",
    "columnRefArray = np.array([np.arange(0, 32) for i in range(numOfRows+5)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.isdir(f\"Models/{MODELNAME}/LRLSTMData\"):\n",
    "    shutil.rmtree(f\"Models/{MODELNAME}/LRLSTMData\")\n",
    "\n",
    "os.mkdir(f\"Models/{MODELNAME}/LRLSTMData\")\n",
    "\n",
    "padSize = rowLength * 3 # lode runner row length * 3 as paper uses previous 3 rows for lstm\n",
    "N = padSize\n",
    "\n",
    "sosArray = np.ones(shape=(1, 256)) * 9\n",
    "eosArray = np.ones(shape=(1, 256)) * 5\n",
    "\n",
    "xTrain = []\n",
    "yTrain = []\n",
    "xTrainTargetIn = []\n",
    "columnRef = []\n",
    "\n",
    "batchNum = 0\n",
    "\n",
    "for i, levelEmbeddingPath in enumerate(lrEmbeddingPaths):\n",
    "\n",
    "    levelEmbeddingArray = np.load(levelEmbeddingPath)\n",
    "\n",
    "    for j in range(len(levelEmbeddingArray) - N):\n",
    "\n",
    "        padLength = (N - j) if j < N else 0\n",
    "        RowCutOff = 0 if j <= N else RowCutOff+1\n",
    "        # print(f\"RowCutOff: {RowCutOff}\")\n",
    "        # print(f\"j: {j}, N: {N}\")\n",
    "        # print(j < N)\n",
    "        # print(padLength)\n",
    "\n",
    "        dataI = np.concatenate((np.zeros(shape=(padLength, 256)), levelEmbeddingArray[RowCutOff:j]), axis=0)\n",
    "        # print(f\"dataI shape: {dataI.shape}\")\n",
    "\n",
    "        dataT = levelEmbeddingArray[j:j+N]\n",
    "        targetIn = np.concatenate((sosArray, dataT))\n",
    "        targetOut = np.concatenate((dataT, eosArray))\n",
    "\n",
    "        #levelIdx = np.concatenate((np.zeros(shape=(padLength)), columnRefArray[RowCutOff:j]), axis=0)\n",
    "        levelIdx = np.array(columnRefArray[j:N+j])\n",
    "        dataC = np.zeros(shape=(N, 256))\n",
    "        for t in range(N): dataC[t][int(levelIdx[t])] = 1\n",
    "        # print(f\"dataC shape: {dataC.shape} : {dataC[-1]}\")\n",
    "\n",
    "        columnRef.append(dataC)\n",
    "        xTrain.append(dataI)\n",
    "        xTrainTargetIn.append(targetIn)\n",
    "        yTrain.append(targetOut)\n",
    "\n",
    "    if len(xTrain) / 32 >= 100.0:\n",
    "\n",
    "        savePath = f\"Models/{MODELNAME}/LRLSTMData/batch{batchNum}\"\n",
    "\n",
    "        np.save(f\"{savePath}xTrain.npy\", np.array(xTrain))\n",
    "        np.save(f\"{savePath}xTrainTargetIn.npy\", np.array(xTrainTargetIn))\n",
    "        np.save(f\"{savePath}yTrain.npy\", np.array(yTrain))\n",
    "        np.save(f\"{savePath}columnRef.npy\", np.array(columnRef))\n",
    "\n",
    "        xTrain = []\n",
    "        yTrain = []\n",
    "        xTrainTargetIn = []\n",
    "        columnRef = []\n",
    "\n",
    "        batchNum += 1\n",
    "\n",
    "if len(xTrain) > 0:\n",
    "\n",
    "    savePath = f\"Models/{MODELNAME}/LRLSTMData/batch{batchNum}\"\n",
    "\n",
    "    np.save(f\"{savePath}xTrain.npy\", np.array(xTrain))\n",
    "    np.save(f\"{savePath}xTrainTargetIn.npy\", np.array(xTrainTargetIn))\n",
    "    np.save(f\"{savePath}yTrain.npy\", np.array(yTrain))\n",
    "    np.save(f\"{savePath}columnRef.npy\", np.array(columnRef))\n",
    "\n",
    "\n",
    "# xTrain = np.array(xTrain)\n",
    "# xTrainTargetIn = np.array(xTrainTargetIn)\n",
    "# yTrain = np.array(yTrain)\n",
    "# columnRef = np.array(columnRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(xTrain, xTrainTargetIn, yTrain, columnRef, epochs, batchSize):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = LSTMModel()\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, eps=1e-7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-7)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    print(f\"xTrain b4 tensor shape: {xTrain.shape}\")\n",
    "    #xTrain = xTrain.reshape(xTrain.shape[0], xTrain.shape[2], xTrain.shape[1])\n",
    "    #xTrainTargetIn = xTrainTargetIn.reshape(xTrainTargetIn.shape[0], xTrainTargetIn.shape[2], xTrainTargetIn.shape[1])\n",
    "    #yTrain = yTrain.reshape(yTrain.shape[0], yTrain.shape[2], yTrain.shape[1])\n",
    "    #columnRef = columnRef.reshape(columnRef.shape[0], columnRef.shape[2], columnRef.shape[1])\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        losses.append([])\n",
    "\n",
    "        for j in range(0, xTrain.shape[0], batchSize):\n",
    "            \n",
    "            xTrainTensor = torch.tensor(xTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "            xTrainTargetInTensor = torch.tensor(xTrainTargetIn[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "            yTrainTensor = torch.tensor(yTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "            columnRefTensor = torch.tensor(columnRef[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "            # print(xTrainTensor.shape)\n",
    "            # print(xTrainTargetInTensor.shape)\n",
    "            # print(yTrainTensor.shape)\n",
    "            # print(columnRefTensor.shape)\n",
    "\n",
    "            #print(f\"xTrain size: {xTrainTensor.size()}\")\n",
    "\n",
    "            yPred = model(xTrainTensor, xTrainTargetInTensor, columnRefTensor)\n",
    "            \n",
    "            #print(f\"yPred size: {yPred.size()}\")\n",
    "            #print(f\"yTruth size: {yTrainTensor.size()}\")\n",
    "            loss = criterion(yPred, yTrainTensor)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses[i].append(loss.cpu().detach().item())\n",
    "\n",
    "        print(f\"Epoch {i}: loss {sum(losses[i])/len(losses[i])}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrainModel(xTrain, xTrainTargetIn, yTrain, columnRef, 25, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils.fastnumpyio as fnp\n",
    "\n",
    "def TrainModelFromFiles(batchPaths, epochs, batchSize, continueTraining=None, learningRate=0.001):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if continueTraining == None:\n",
    "        model = LSTMModel()\n",
    "    else:\n",
    "        model = continueTraining\n",
    "    \n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learningRate, eps=1e-7)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, eps=1e-7)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        losses.append([])\n",
    "\n",
    "        for t in range(0, len(batchPaths), 4):\n",
    "            print(t)\n",
    "            # print(batchPaths[t:t+4])\n",
    "\n",
    "            xTrain = fnp.load(batchPaths[t+1])\n",
    "            xTrainTargetIn = fnp.load(batchPaths[t+2])\n",
    "            yTrain = fnp.load(batchPaths[t+3])\n",
    "            columnRef = fnp.load(batchPaths[t])\n",
    "\n",
    "            for j in range(0, xTrain.shape[0], batchSize):\n",
    "                \n",
    "                xTrainTensor = torch.tensor(xTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "                xTrainTargetInTensor = torch.tensor(xTrainTargetIn[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                yTrainTensor = torch.tensor(yTrain[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                columnRefTensor = torch.tensor(columnRef[j:j+batchSize], dtype=torch.float32).to(device)\n",
    "\n",
    "                # print(xTrainTensor.shape)\n",
    "                # print(xTrainTargetInTensor.shape)\n",
    "                # print(yTrainTensor.shape)\n",
    "                # print(columnRefTensor.shape)\n",
    "\n",
    "                #print(f\"xTrain size: {xTrainTensor.size()}\")\n",
    "\n",
    "                yPred = model(xTrainTensor, xTrainTargetInTensor, columnRefTensor)\n",
    "                \n",
    "                #print(f\"yPred size: {yPred.size()}\")\n",
    "                #print(f\"yTruth size: {yTrainTensor.size()}\")\n",
    "                loss = criterion(yPred, yTrainTensor)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses[i].append(loss.cpu().detach().item())\n",
    "\n",
    "            print(f\"Epoch {i} Batch {t}: loss {losses[i][-1]}\")\n",
    "\n",
    "        print(f\"Epoch {i}: loss {sum(losses[i])/len(losses[i])}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0 Batch 0: loss 0.7844229936599731\n",
      "4\n",
      "Epoch 0 Batch 4: loss 0.4697379469871521\n",
      "8\n",
      "Epoch 0 Batch 8: loss 0.5302704572677612\n",
      "12\n",
      "Epoch 0 Batch 12: loss 0.40791139006614685\n",
      "16\n",
      "Epoch 0 Batch 16: loss 0.667203426361084\n",
      "20\n",
      "Epoch 0 Batch 20: loss 0.6138553023338318\n",
      "24\n",
      "Epoch 0 Batch 24: loss 0.8279170393943787\n",
      "28\n",
      "Epoch 0 Batch 28: loss 0.4224819242954254\n",
      "32\n",
      "Epoch 0 Batch 32: loss 0.4135683476924896\n",
      "36\n",
      "Epoch 0 Batch 36: loss 0.7125810980796814\n",
      "40\n",
      "Epoch 0 Batch 40: loss 0.5643162727355957\n",
      "44\n",
      "Epoch 0 Batch 44: loss 0.5660424828529358\n",
      "48\n",
      "Epoch 0 Batch 48: loss 0.5085227489471436\n",
      "52\n",
      "Epoch 0 Batch 52: loss 0.5907588005065918\n",
      "56\n",
      "Epoch 0 Batch 56: loss 0.7101013660430908\n",
      "60\n",
      "Epoch 0 Batch 60: loss 0.6770675182342529\n",
      "64\n",
      "Epoch 0 Batch 64: loss 0.6672075390815735\n",
      "68\n",
      "Epoch 0 Batch 68: loss 0.7162806987762451\n",
      "72\n",
      "Epoch 0 Batch 72: loss 0.7146070003509521\n",
      "76\n",
      "Epoch 0 Batch 76: loss 0.514797568321228\n",
      "80\n",
      "Epoch 0 Batch 80: loss 0.6037762761116028\n",
      "84\n",
      "Epoch 0 Batch 84: loss 0.6199766397476196\n",
      "88\n",
      "Epoch 0 Batch 88: loss 0.5632084012031555\n",
      "92\n",
      "Epoch 0 Batch 92: loss 0.7208173274993896\n",
      "96\n",
      "Epoch 0 Batch 96: loss 0.5237880945205688\n",
      "Epoch 0: loss 0.6158993868242231\n",
      "0\n",
      "Epoch 1 Batch 0: loss 0.7738263607025146\n",
      "4\n",
      "Epoch 1 Batch 4: loss 0.46204492449760437\n",
      "8\n",
      "Epoch 1 Batch 8: loss 0.5144578218460083\n",
      "12\n",
      "Epoch 1 Batch 12: loss 0.40756019949913025\n",
      "16\n",
      "Epoch 1 Batch 16: loss 0.660563051700592\n",
      "20\n",
      "Epoch 1 Batch 20: loss 0.6119999885559082\n",
      "24\n",
      "Epoch 1 Batch 24: loss 0.8075734972953796\n",
      "28\n",
      "Epoch 1 Batch 28: loss 0.4170760214328766\n",
      "32\n",
      "Epoch 1 Batch 32: loss 0.40900105237960815\n",
      "36\n",
      "Epoch 1 Batch 36: loss 0.6817474961280823\n",
      "40\n",
      "Epoch 1 Batch 40: loss 0.5586521029472351\n",
      "44\n",
      "Epoch 1 Batch 44: loss 0.5487101078033447\n",
      "48\n",
      "Epoch 1 Batch 48: loss 0.483301043510437\n",
      "52\n",
      "Epoch 1 Batch 52: loss 0.5592126846313477\n",
      "56\n",
      "Epoch 1 Batch 56: loss 0.6569067239761353\n",
      "60\n",
      "Epoch 1 Batch 60: loss 0.6292600035667419\n",
      "64\n",
      "Epoch 1 Batch 64: loss 0.6130568981170654\n",
      "68\n",
      "Epoch 1 Batch 68: loss 0.6563400626182556\n",
      "72\n",
      "Epoch 1 Batch 72: loss 0.6438191533088684\n",
      "76\n",
      "Epoch 1 Batch 76: loss 0.4488319158554077\n",
      "80\n",
      "Epoch 1 Batch 80: loss 0.5389894247055054\n",
      "84\n",
      "Epoch 1 Batch 84: loss 0.5741490721702576\n",
      "88\n",
      "Epoch 1 Batch 88: loss 0.49478650093078613\n",
      "92\n",
      "Epoch 1 Batch 92: loss 0.6593541502952576\n",
      "96\n",
      "Epoch 1 Batch 96: loss 0.4640188217163086\n",
      "Epoch 1: loss 0.5804091387307435\n",
      "0\n",
      "Epoch 2 Batch 0: loss 0.7102431058883667\n",
      "4\n",
      "Epoch 2 Batch 4: loss 0.3897112011909485\n",
      "8\n",
      "Epoch 2 Batch 8: loss 0.43502113223075867\n",
      "12\n",
      "Epoch 2 Batch 12: loss 0.32533544301986694\n",
      "16\n",
      "Epoch 2 Batch 16: loss 0.5857146978378296\n",
      "20\n",
      "Epoch 2 Batch 20: loss 0.5334571003913879\n",
      "24\n",
      "Epoch 2 Batch 24: loss 0.7452793717384338\n",
      "28\n",
      "Epoch 2 Batch 28: loss 0.34363728761672974\n",
      "32\n",
      "Epoch 2 Batch 32: loss 0.3338329493999481\n",
      "36\n",
      "Epoch 2 Batch 36: loss 0.6157431602478027\n",
      "40\n",
      "Epoch 2 Batch 40: loss 0.46787747740745544\n",
      "44\n",
      "Epoch 2 Batch 44: loss 0.47330787777900696\n",
      "48\n",
      "Epoch 2 Batch 48: loss 0.4083818793296814\n",
      "52\n",
      "Epoch 2 Batch 52: loss 0.501596212387085\n",
      "56\n",
      "Epoch 2 Batch 56: loss 0.62019282579422\n",
      "60\n",
      "Epoch 2 Batch 60: loss 0.5867289900779724\n",
      "64\n",
      "Epoch 2 Batch 64: loss 0.5845618844032288\n",
      "68\n",
      "Epoch 2 Batch 68: loss 0.5635422468185425\n",
      "72\n",
      "Epoch 2 Batch 72: loss 0.6324867606163025\n",
      "76\n",
      "Epoch 2 Batch 76: loss 0.4271494150161743\n",
      "80\n",
      "Epoch 2 Batch 80: loss 0.5216060280799866\n",
      "84\n",
      "Epoch 2 Batch 84: loss 0.5371782779693604\n",
      "88\n",
      "Epoch 2 Batch 88: loss 0.48035451769828796\n",
      "92\n",
      "Epoch 2 Batch 92: loss 0.6347286105155945\n",
      "96\n",
      "Epoch 2 Batch 96: loss 0.43626394867897034\n",
      "Epoch 2: loss 0.5249037191480921\n",
      "0\n",
      "Epoch 3 Batch 0: loss 0.6946129202842712\n",
      "4\n",
      "Epoch 3 Batch 4: loss 0.3795388340950012\n",
      "8\n",
      "Epoch 3 Batch 8: loss 0.4374035596847534\n",
      "12\n",
      "Epoch 3 Batch 12: loss 0.32321321964263916\n",
      "16\n",
      "Epoch 3 Batch 16: loss 0.5893515944480896\n",
      "20\n",
      "Epoch 3 Batch 20: loss 0.5270065665245056\n",
      "24\n",
      "Epoch 3 Batch 24: loss 0.7511082291603088\n",
      "28\n",
      "Epoch 3 Batch 28: loss 0.332988440990448\n",
      "32\n",
      "Epoch 3 Batch 32: loss 0.3290252089500427\n",
      "36\n",
      "Epoch 3 Batch 36: loss 0.6331060528755188\n",
      "40\n",
      "Epoch 3 Batch 40: loss 0.4639315605163574\n",
      "44\n",
      "Epoch 3 Batch 44: loss 0.47041815519332886\n",
      "48\n",
      "Epoch 3 Batch 48: loss 0.4128805696964264\n",
      "52\n",
      "Epoch 3 Batch 52: loss 0.5133547186851501\n",
      "56\n",
      "Epoch 3 Batch 56: loss 0.6157517433166504\n",
      "60\n",
      "Epoch 3 Batch 60: loss 0.6073000431060791\n",
      "64\n",
      "Epoch 3 Batch 64: loss 0.5900534391403198\n",
      "68\n",
      "Epoch 3 Batch 68: loss 0.6319235563278198\n",
      "72\n",
      "Epoch 3 Batch 72: loss 0.6435839533805847\n",
      "76\n",
      "Epoch 3 Batch 76: loss 0.42452922463417053\n",
      "80\n",
      "Epoch 3 Batch 80: loss 0.5255504846572876\n",
      "84\n",
      "Epoch 3 Batch 84: loss 0.5473635792732239\n",
      "88\n",
      "Epoch 3 Batch 88: loss 0.48208168148994446\n",
      "92\n",
      "Epoch 3 Batch 92: loss 0.6396183967590332\n",
      "96\n",
      "Epoch 3 Batch 96: loss 0.4484354257583618\n",
      "Epoch 3: loss 0.52129521215694\n",
      "0\n",
      "Epoch 4 Batch 0: loss 0.6987482309341431\n",
      "4\n",
      "Epoch 4 Batch 4: loss 0.37750816345214844\n",
      "8\n",
      "Epoch 4 Batch 8: loss 0.44994255900382996\n",
      "12\n",
      "Epoch 4 Batch 12: loss 0.31592056155204773\n",
      "16\n",
      "Epoch 4 Batch 16: loss 0.5693337917327881\n",
      "20\n",
      "Epoch 4 Batch 20: loss 0.5325444340705872\n",
      "24\n",
      "Epoch 4 Batch 24: loss 0.7470113635063171\n",
      "28\n",
      "Epoch 4 Batch 28: loss 0.33493804931640625\n",
      "32\n",
      "Epoch 4 Batch 32: loss 0.32967469096183777\n",
      "36\n",
      "Epoch 4 Batch 36: loss 0.5914967656135559\n",
      "40\n",
      "Epoch 4 Batch 40: loss 0.4625357687473297\n",
      "44\n",
      "Epoch 4 Batch 44: loss 0.46972769498825073\n",
      "48\n",
      "Epoch 4 Batch 48: loss 0.41130468249320984\n",
      "52\n",
      "Epoch 4 Batch 52: loss 0.5144240260124207\n",
      "56\n",
      "Epoch 4 Batch 56: loss 0.6177359819412231\n",
      "60\n",
      "Epoch 4 Batch 60: loss 0.6016768217086792\n",
      "64\n",
      "Epoch 4 Batch 64: loss 0.6012328267097473\n",
      "68\n",
      "Epoch 4 Batch 68: loss 0.6156083345413208\n",
      "72\n",
      "Epoch 4 Batch 72: loss 0.6132252216339111\n",
      "76\n",
      "Epoch 4 Batch 76: loss 0.42152756452560425\n",
      "80\n",
      "Epoch 4 Batch 80: loss 0.5199600458145142\n",
      "84\n",
      "Epoch 4 Batch 84: loss 0.5132147073745728\n",
      "88\n",
      "Epoch 4 Batch 88: loss 0.48396503925323486\n",
      "92\n",
      "Epoch 4 Batch 92: loss 0.6315929293632507\n",
      "96\n",
      "Epoch 4 Batch 96: loss 0.4380538761615753\n",
      "Epoch 4: loss 0.5200480400836258\n",
      "0\n",
      "Epoch 5 Batch 0: loss 0.7006398439407349\n",
      "4\n",
      "Epoch 5 Batch 4: loss 0.3786904513835907\n",
      "8\n",
      "Epoch 5 Batch 8: loss 0.4301895201206207\n",
      "12\n",
      "Epoch 5 Batch 12: loss 0.33171507716178894\n",
      "16\n",
      "Epoch 5 Batch 16: loss 0.5457766056060791\n",
      "20\n",
      "Epoch 5 Batch 20: loss 0.5459194779396057\n",
      "24\n",
      "Epoch 5 Batch 24: loss 0.7417261600494385\n",
      "28\n",
      "Epoch 5 Batch 28: loss 0.3275555670261383\n",
      "32\n",
      "Epoch 5 Batch 32: loss 0.3231904208660126\n",
      "36\n",
      "Epoch 5 Batch 36: loss 0.6044819355010986\n",
      "40\n",
      "Epoch 5 Batch 40: loss 0.4585369825363159\n",
      "44\n",
      "Epoch 5 Batch 44: loss 0.46291160583496094\n",
      "48\n",
      "Epoch 5 Batch 48: loss 0.41098257899284363\n",
      "52\n",
      "Epoch 5 Batch 52: loss 0.48871248960494995\n",
      "56\n",
      "Epoch 5 Batch 56: loss 0.5800975561141968\n",
      "60\n",
      "Epoch 5 Batch 60: loss 0.5937053561210632\n",
      "64\n",
      "Epoch 5 Batch 64: loss 0.5796486735343933\n",
      "68\n",
      "Epoch 5 Batch 68: loss 0.5961522459983826\n",
      "72\n",
      "Epoch 5 Batch 72: loss 0.6415892839431763\n",
      "76\n",
      "Epoch 5 Batch 76: loss 0.4197038412094116\n",
      "80\n",
      "Epoch 5 Batch 80: loss 0.5249233841896057\n",
      "84\n",
      "Epoch 5 Batch 84: loss 0.5121316909790039\n",
      "88\n",
      "Epoch 5 Batch 88: loss 0.4710434377193451\n",
      "92\n",
      "Epoch 5 Batch 92: loss 0.6341865062713623\n",
      "96\n",
      "Epoch 5 Batch 96: loss 0.4467482268810272\n",
      "Epoch 5: loss 0.5174911014918695\n",
      "0\n",
      "Epoch 6 Batch 0: loss 0.6959816813468933\n",
      "4\n",
      "Epoch 6 Batch 4: loss 0.37031230330467224\n",
      "8\n",
      "Epoch 6 Batch 8: loss 0.42409074306488037\n",
      "12\n",
      "Epoch 6 Batch 12: loss 0.3113638460636139\n",
      "16\n",
      "Epoch 6 Batch 16: loss 0.5690356492996216\n",
      "20\n",
      "Epoch 6 Batch 20: loss 0.5448602437973022\n",
      "24\n",
      "Epoch 6 Batch 24: loss 0.7476884126663208\n",
      "28\n",
      "Epoch 6 Batch 28: loss 0.3277183473110199\n",
      "32\n",
      "Epoch 6 Batch 32: loss 0.31775709986686707\n",
      "36\n",
      "Epoch 6 Batch 36: loss 0.6091804504394531\n",
      "40\n",
      "Epoch 6 Batch 40: loss 0.4604155421257019\n",
      "44\n",
      "Epoch 6 Batch 44: loss 0.4803157448768616\n",
      "48\n",
      "Epoch 6 Batch 48: loss 0.40616726875305176\n",
      "52\n",
      "Epoch 6 Batch 52: loss 0.48591211438179016\n",
      "56\n",
      "Epoch 6 Batch 56: loss 0.5973713994026184\n",
      "60\n",
      "Epoch 6 Batch 60: loss 0.607257604598999\n",
      "64\n",
      "Epoch 6 Batch 64: loss 0.5820754170417786\n",
      "68\n",
      "Epoch 6 Batch 68: loss 0.6344093680381775\n",
      "72\n",
      "Epoch 6 Batch 72: loss 0.6177676916122437\n",
      "76\n",
      "Epoch 6 Batch 76: loss 0.42230379581451416\n",
      "80\n",
      "Epoch 6 Batch 80: loss 0.51336669921875\n",
      "84\n",
      "Epoch 6 Batch 84: loss 0.5363849401473999\n",
      "88\n",
      "Epoch 6 Batch 88: loss 0.4744556248188019\n",
      "92\n",
      "Epoch 6 Batch 92: loss 0.6533830761909485\n",
      "96\n",
      "Epoch 6 Batch 96: loss 0.44480186700820923\n",
      "Epoch 6: loss 0.5179082470400291\n",
      "0\n",
      "Epoch 7 Batch 0: loss 0.7070225477218628\n",
      "4\n",
      "Epoch 7 Batch 4: loss 0.3749535381793976\n",
      "8\n",
      "Epoch 7 Batch 8: loss 0.4265598952770233\n",
      "12\n",
      "Epoch 7 Batch 12: loss 0.32274752855300903\n",
      "16\n",
      "Epoch 7 Batch 16: loss 0.5724985599517822\n",
      "20\n",
      "Epoch 7 Batch 20: loss 0.5284601449966431\n",
      "24\n",
      "Epoch 7 Batch 24: loss 0.768286943435669\n",
      "28\n",
      "Epoch 7 Batch 28: loss 0.3307259976863861\n",
      "32\n",
      "Epoch 7 Batch 32: loss 0.3252091109752655\n",
      "36\n",
      "Epoch 7 Batch 36: loss 0.5885753035545349\n",
      "40\n",
      "Epoch 7 Batch 40: loss 0.4619938135147095\n",
      "44\n",
      "Epoch 7 Batch 44: loss 0.4923102557659149\n",
      "48\n",
      "Epoch 7 Batch 48: loss 0.40572261810302734\n",
      "52\n",
      "Epoch 7 Batch 52: loss 0.49125149846076965\n",
      "56\n",
      "Epoch 7 Batch 56: loss 0.5913490653038025\n",
      "60\n",
      "Epoch 7 Batch 60: loss 0.600570797920227\n",
      "64\n",
      "Epoch 7 Batch 64: loss 0.6051074862480164\n",
      "68\n",
      "Epoch 7 Batch 68: loss 0.6355530619621277\n",
      "72\n",
      "Epoch 7 Batch 72: loss 0.6014013886451721\n",
      "76\n",
      "Epoch 7 Batch 76: loss 0.42052769660949707\n",
      "80\n",
      "Epoch 7 Batch 80: loss 0.5191673636436462\n",
      "84\n",
      "Epoch 7 Batch 84: loss 0.4648889899253845\n",
      "88\n",
      "Epoch 7 Batch 88: loss 0.4726337492465973\n",
      "92\n",
      "Epoch 7 Batch 92: loss 0.6257015466690063\n",
      "96\n",
      "Epoch 7 Batch 96: loss 0.448720782995224\n",
      "Epoch 7: loss 0.5193353286437821\n",
      "0\n",
      "Epoch 8 Batch 0: loss 0.7150609493255615\n",
      "4\n",
      "Epoch 8 Batch 4: loss 0.3707544207572937\n",
      "8\n",
      "Epoch 8 Batch 8: loss 0.4242897629737854\n",
      "12\n",
      "Epoch 8 Batch 12: loss 0.3335763216018677\n",
      "16\n",
      "Epoch 8 Batch 16: loss 0.5868210196495056\n",
      "20\n",
      "Epoch 8 Batch 20: loss 0.5259982347488403\n",
      "24\n",
      "Epoch 8 Batch 24: loss 0.7565261721611023\n",
      "28\n",
      "Epoch 8 Batch 28: loss 0.3319157063961029\n",
      "32\n",
      "Epoch 8 Batch 32: loss 0.3258458971977234\n",
      "36\n",
      "Epoch 8 Batch 36: loss 0.574749767780304\n",
      "40\n",
      "Epoch 8 Batch 40: loss 0.4565542936325073\n",
      "44\n",
      "Epoch 8 Batch 44: loss 0.48024192452430725\n",
      "48\n",
      "Epoch 8 Batch 48: loss 0.4051339626312256\n",
      "52\n",
      "Epoch 8 Batch 52: loss 0.5006208419799805\n",
      "56\n",
      "Epoch 8 Batch 56: loss 0.5731738805770874\n",
      "60\n",
      "Epoch 8 Batch 60: loss 0.5971614122390747\n",
      "64\n",
      "Epoch 8 Batch 64: loss 0.5848020911216736\n",
      "68\n",
      "Epoch 8 Batch 68: loss 0.628343403339386\n",
      "72\n",
      "Epoch 8 Batch 72: loss 0.6271545886993408\n",
      "76\n",
      "Epoch 8 Batch 76: loss 0.42237135767936707\n",
      "80\n",
      "Epoch 8 Batch 80: loss 0.5205199718475342\n",
      "84\n",
      "Epoch 8 Batch 84: loss 0.5558307766914368\n",
      "88\n",
      "Epoch 8 Batch 88: loss 0.4697304666042328\n",
      "92\n",
      "Epoch 8 Batch 92: loss 0.6433380842208862\n",
      "96\n",
      "Epoch 8 Batch 96: loss 0.44031384587287903\n",
      "Epoch 8: loss 0.5200069723526637\n",
      "0\n",
      "Epoch 9 Batch 0: loss 0.6829307079315186\n",
      "4\n",
      "Epoch 9 Batch 4: loss 0.37344667315483093\n",
      "8\n",
      "Epoch 9 Batch 8: loss 0.41644978523254395\n",
      "12\n",
      "Epoch 9 Batch 12: loss 0.3215644061565399\n",
      "16\n",
      "Epoch 9 Batch 16: loss 0.5653061270713806\n",
      "20\n",
      "Epoch 9 Batch 20: loss 0.5201530456542969\n",
      "24\n",
      "Epoch 9 Batch 24: loss 0.7438060641288757\n",
      "28\n",
      "Epoch 9 Batch 28: loss 0.3274902105331421\n",
      "32\n",
      "Epoch 9 Batch 32: loss 0.334568053483963\n",
      "36\n",
      "Epoch 9 Batch 36: loss 0.5885986685752869\n",
      "40\n",
      "Epoch 9 Batch 40: loss 0.45260947942733765\n",
      "44\n",
      "Epoch 9 Batch 44: loss 0.4707076847553253\n",
      "48\n",
      "Epoch 9 Batch 48: loss 0.40213605761528015\n",
      "52\n",
      "Epoch 9 Batch 52: loss 0.5110927224159241\n",
      "56\n",
      "Epoch 9 Batch 56: loss 0.6000716686248779\n",
      "60\n",
      "Epoch 9 Batch 60: loss 0.6192297339439392\n",
      "64\n",
      "Epoch 9 Batch 64: loss 0.5904401540756226\n",
      "68\n",
      "Epoch 9 Batch 68: loss 0.6165356636047363\n",
      "72\n",
      "Epoch 9 Batch 72: loss 0.6098638772964478\n",
      "76\n",
      "Epoch 9 Batch 76: loss 0.4260577857494354\n",
      "80\n",
      "Epoch 9 Batch 80: loss 0.5101615786552429\n",
      "84\n",
      "Epoch 9 Batch 84: loss 0.5132676362991333\n",
      "88\n",
      "Epoch 9 Batch 88: loss 0.4786913990974426\n",
      "92\n",
      "Epoch 9 Batch 92: loss 0.6289025545120239\n",
      "96\n",
      "Epoch 9 Batch 96: loss 0.4474530518054962\n",
      "Epoch 9: loss 0.5181026755665478\n",
      "0\n",
      "Epoch 10 Batch 0: loss 0.7056276798248291\n",
      "4\n",
      "Epoch 10 Batch 4: loss 0.3714952766895294\n",
      "8\n",
      "Epoch 10 Batch 8: loss 0.430505633354187\n",
      "12\n",
      "Epoch 10 Batch 12: loss 0.3275766670703888\n",
      "16\n",
      "Epoch 10 Batch 16: loss 0.562286376953125\n",
      "20\n",
      "Epoch 10 Batch 20: loss 0.5317897200584412\n",
      "24\n",
      "Epoch 10 Batch 24: loss 0.7556339502334595\n",
      "28\n",
      "Epoch 10 Batch 28: loss 0.3317265212535858\n",
      "32\n",
      "Epoch 10 Batch 32: loss 0.32891225814819336\n",
      "36\n",
      "Epoch 10 Batch 36: loss 0.614339292049408\n",
      "40\n",
      "Epoch 10 Batch 40: loss 0.457969605922699\n",
      "44\n",
      "Epoch 10 Batch 44: loss 0.4798114001750946\n",
      "48\n",
      "Epoch 10 Batch 48: loss 0.4173213541507721\n",
      "52\n",
      "Epoch 10 Batch 52: loss 0.48923081159591675\n",
      "56\n",
      "Epoch 10 Batch 56: loss 0.5606204271316528\n",
      "60\n",
      "Epoch 10 Batch 60: loss 0.5962885022163391\n",
      "64\n",
      "Epoch 10 Batch 64: loss 0.5788527727127075\n",
      "68\n",
      "Epoch 10 Batch 68: loss 0.6601113677024841\n",
      "72\n",
      "Epoch 10 Batch 72: loss 0.6293488144874573\n",
      "76\n",
      "Epoch 10 Batch 76: loss 0.42502743005752563\n",
      "80\n",
      "Epoch 10 Batch 80: loss 0.530769944190979\n",
      "84\n",
      "Epoch 10 Batch 84: loss 0.5297728180885315\n",
      "88\n",
      "Epoch 10 Batch 88: loss 0.479256808757782\n",
      "92\n",
      "Epoch 10 Batch 92: loss 0.6272614598274231\n",
      "96\n",
      "Epoch 10 Batch 96: loss 0.4427407383918762\n",
      "Epoch 10: loss 0.5195852575594919\n",
      "0\n",
      "Epoch 11 Batch 0: loss 0.7055058479309082\n",
      "4\n",
      "Epoch 11 Batch 4: loss 0.3727972209453583\n",
      "8\n",
      "Epoch 11 Batch 8: loss 0.42683497071266174\n",
      "12\n",
      "Epoch 11 Batch 12: loss 0.33491453528404236\n",
      "16\n",
      "Epoch 11 Batch 16: loss 0.5680602192878723\n",
      "20\n",
      "Epoch 11 Batch 20: loss 0.525104284286499\n",
      "24\n",
      "Epoch 11 Batch 24: loss 0.7345532178878784\n",
      "28\n",
      "Epoch 11 Batch 28: loss 0.32608217000961304\n",
      "32\n",
      "Epoch 11 Batch 32: loss 0.32065871357917786\n",
      "36\n",
      "Epoch 11 Batch 36: loss 0.6068421006202698\n",
      "40\n",
      "Epoch 11 Batch 40: loss 0.4581831693649292\n",
      "44\n",
      "Epoch 11 Batch 44: loss 0.4684571623802185\n",
      "48\n",
      "Epoch 11 Batch 48: loss 0.4269609749317169\n",
      "52\n",
      "Epoch 11 Batch 52: loss 0.5166218280792236\n",
      "56\n",
      "Epoch 11 Batch 56: loss 0.5485212206840515\n",
      "60\n",
      "Epoch 11 Batch 60: loss 0.5940311551094055\n",
      "64\n",
      "Epoch 11 Batch 64: loss 0.5795141458511353\n",
      "68\n",
      "Epoch 11 Batch 68: loss 0.6484382748603821\n",
      "72\n",
      "Epoch 11 Batch 72: loss 0.6059463024139404\n",
      "76\n",
      "Epoch 11 Batch 76: loss 0.4267902076244354\n",
      "80\n",
      "Epoch 11 Batch 80: loss 0.5096081495285034\n",
      "84\n",
      "Epoch 11 Batch 84: loss 0.5115507245063782\n",
      "88\n",
      "Epoch 11 Batch 88: loss 0.4757007658481598\n",
      "92\n",
      "Epoch 11 Batch 92: loss 0.6233838200569153\n",
      "96\n",
      "Epoch 11 Batch 96: loss 0.4386843144893646\n",
      "Epoch 11: loss 0.5185618335897463\n",
      "0\n",
      "Epoch 12 Batch 0: loss 0.7013847827911377\n",
      "4\n",
      "Epoch 12 Batch 4: loss 0.3725472688674927\n",
      "8\n",
      "Epoch 12 Batch 8: loss 0.4383872449398041\n",
      "12\n",
      "Epoch 12 Batch 12: loss 0.3394981324672699\n",
      "16\n",
      "Epoch 12 Batch 16: loss 0.5791696310043335\n",
      "20\n",
      "Epoch 12 Batch 20: loss 0.5367594957351685\n",
      "24\n",
      "Epoch 12 Batch 24: loss 0.7386615872383118\n",
      "28\n",
      "Epoch 12 Batch 28: loss 0.34018203616142273\n",
      "32\n",
      "Epoch 12 Batch 32: loss 0.3329443633556366\n",
      "36\n",
      "Epoch 12 Batch 36: loss 0.5900608897209167\n",
      "40\n",
      "Epoch 12 Batch 40: loss 0.4733469784259796\n",
      "44\n",
      "Epoch 12 Batch 44: loss 0.47083956003189087\n",
      "48\n",
      "Epoch 12 Batch 48: loss 0.4165012240409851\n",
      "52\n",
      "Epoch 12 Batch 52: loss 0.4903873801231384\n",
      "56\n",
      "Epoch 12 Batch 56: loss 0.5684268474578857\n",
      "60\n",
      "Epoch 12 Batch 60: loss 0.599087119102478\n",
      "64\n",
      "Epoch 12 Batch 64: loss 0.5821157097816467\n",
      "68\n",
      "Epoch 12 Batch 68: loss 0.6325197219848633\n",
      "72\n",
      "Epoch 12 Batch 72: loss 0.6278011202812195\n",
      "76\n",
      "Epoch 12 Batch 76: loss 0.4190104901790619\n",
      "80\n",
      "Epoch 12 Batch 80: loss 0.5330637097358704\n",
      "84\n",
      "Epoch 12 Batch 84: loss 0.5281213521957397\n",
      "88\n",
      "Epoch 12 Batch 88: loss 0.4791715145111084\n",
      "92\n",
      "Epoch 12 Batch 92: loss 0.6129049062728882\n",
      "96\n",
      "Epoch 12 Batch 96: loss 0.4431343674659729\n",
      "Epoch 12: loss 0.5206915959506704\n",
      "0\n",
      "Epoch 13 Batch 0: loss 0.6985607147216797\n",
      "4\n",
      "Epoch 13 Batch 4: loss 0.37159624695777893\n",
      "8\n",
      "Epoch 13 Batch 8: loss 0.4416455924510956\n",
      "12\n",
      "Epoch 13 Batch 12: loss 0.32080352306365967\n",
      "16\n",
      "Epoch 13 Batch 16: loss 0.5686438679695129\n",
      "20\n",
      "Epoch 13 Batch 20: loss 0.532623827457428\n",
      "24\n",
      "Epoch 13 Batch 24: loss 0.740582287311554\n",
      "28\n",
      "Epoch 13 Batch 28: loss 0.33783137798309326\n",
      "32\n",
      "Epoch 13 Batch 32: loss 0.32297274470329285\n",
      "36\n",
      "Epoch 13 Batch 36: loss 0.6050398349761963\n",
      "40\n",
      "Epoch 13 Batch 40: loss 0.4710093140602112\n",
      "44\n",
      "Epoch 13 Batch 44: loss 0.4681762158870697\n",
      "48\n",
      "Epoch 13 Batch 48: loss 0.4122656583786011\n",
      "52\n",
      "Epoch 13 Batch 52: loss 0.47703394293785095\n",
      "56\n",
      "Epoch 13 Batch 56: loss 0.5467658042907715\n",
      "60\n",
      "Epoch 13 Batch 60: loss 0.6035118103027344\n",
      "64\n",
      "Epoch 13 Batch 64: loss 0.5867661237716675\n",
      "68\n",
      "Epoch 13 Batch 68: loss 0.6460164189338684\n",
      "72\n",
      "Epoch 13 Batch 72: loss 0.6142856478691101\n",
      "76\n",
      "Epoch 13 Batch 76: loss 0.42895257472991943\n",
      "80\n",
      "Epoch 13 Batch 80: loss 0.5237484574317932\n",
      "84\n",
      "Epoch 13 Batch 84: loss 0.5355716943740845\n",
      "88\n",
      "Epoch 13 Batch 88: loss 0.4687711298465729\n",
      "92\n",
      "Epoch 13 Batch 92: loss 0.6299615502357483\n",
      "96\n",
      "Epoch 13 Batch 96: loss 0.44067052006721497\n",
      "Epoch 13: loss 0.5207154838819252\n",
      "0\n",
      "Epoch 14 Batch 0: loss 0.7051849365234375\n",
      "4\n",
      "Epoch 14 Batch 4: loss 0.3679877817630768\n",
      "8\n",
      "Epoch 14 Batch 8: loss 0.4300432503223419\n",
      "12\n",
      "Epoch 14 Batch 12: loss 0.331455796957016\n",
      "16\n",
      "Epoch 14 Batch 16: loss 0.5889992117881775\n",
      "20\n",
      "Epoch 14 Batch 20: loss 0.5370529294013977\n",
      "24\n",
      "Epoch 14 Batch 24: loss 0.754572868347168\n",
      "28\n",
      "Epoch 14 Batch 28: loss 0.3298283815383911\n",
      "32\n",
      "Epoch 14 Batch 32: loss 0.321626752614975\n",
      "36\n",
      "Epoch 14 Batch 36: loss 0.5840146541595459\n",
      "40\n",
      "Epoch 14 Batch 40: loss 0.4692700505256653\n",
      "44\n",
      "Epoch 14 Batch 44: loss 0.47574079036712646\n",
      "48\n",
      "Epoch 14 Batch 48: loss 0.41214290261268616\n",
      "52\n",
      "Epoch 14 Batch 52: loss 0.44867387413978577\n",
      "56\n",
      "Epoch 14 Batch 56: loss 0.5978723168373108\n",
      "60\n",
      "Epoch 14 Batch 60: loss 0.5836987495422363\n",
      "64\n",
      "Epoch 14 Batch 64: loss 0.570513904094696\n",
      "68\n",
      "Epoch 14 Batch 68: loss 0.6046784520149231\n",
      "72\n",
      "Epoch 14 Batch 72: loss 0.6239066123962402\n",
      "76\n",
      "Epoch 14 Batch 76: loss 0.4264369010925293\n",
      "80\n",
      "Epoch 14 Batch 80: loss 0.5185688138008118\n",
      "84\n",
      "Epoch 14 Batch 84: loss 0.5259099006652832\n",
      "88\n",
      "Epoch 14 Batch 88: loss 0.47732630372047424\n",
      "92\n",
      "Epoch 14 Batch 92: loss 0.6310463547706604\n",
      "96\n",
      "Epoch 14 Batch 96: loss 0.4386042356491089\n",
      "Epoch 14: loss 0.5189749289866079\n",
      "0\n",
      "Epoch 15 Batch 0: loss 0.6951368451118469\n",
      "4\n",
      "Epoch 15 Batch 4: loss 0.37321868538856506\n",
      "8\n",
      "Epoch 15 Batch 8: loss 0.4305991232395172\n",
      "12\n",
      "Epoch 15 Batch 12: loss 0.33766308426856995\n",
      "16\n",
      "Epoch 15 Batch 16: loss 0.5759549140930176\n",
      "20\n",
      "Epoch 15 Batch 20: loss 0.5337515473365784\n",
      "24\n",
      "Epoch 15 Batch 24: loss 0.7432342171669006\n",
      "28\n",
      "Epoch 15 Batch 28: loss 0.33071768283843994\n",
      "32\n",
      "Epoch 15 Batch 32: loss 0.3256680369377136\n",
      "36\n",
      "Epoch 15 Batch 36: loss 0.6015170812606812\n",
      "40\n",
      "Epoch 15 Batch 40: loss 0.46492892503738403\n",
      "44\n",
      "Epoch 15 Batch 44: loss 0.4627281427383423\n",
      "48\n",
      "Epoch 15 Batch 48: loss 0.40737292170524597\n",
      "52\n",
      "Epoch 15 Batch 52: loss 0.47367367148399353\n",
      "56\n",
      "Epoch 15 Batch 56: loss 0.5525022745132446\n",
      "60\n",
      "Epoch 15 Batch 60: loss 0.5990020036697388\n",
      "64\n",
      "Epoch 15 Batch 64: loss 0.5848639011383057\n",
      "68\n",
      "Epoch 15 Batch 68: loss 0.5823696255683899\n",
      "72\n",
      "Epoch 15 Batch 72: loss 0.6381786465644836\n",
      "76\n",
      "Epoch 15 Batch 76: loss 0.42497333884239197\n",
      "80\n",
      "Epoch 15 Batch 80: loss 0.5142338871955872\n",
      "84\n",
      "Epoch 15 Batch 84: loss 0.5103002786636353\n",
      "88\n",
      "Epoch 15 Batch 88: loss 0.481540322303772\n",
      "92\n",
      "Epoch 15 Batch 92: loss 0.6304422616958618\n",
      "96\n",
      "Epoch 15 Batch 96: loss 0.4447244703769684\n",
      "Epoch 15: loss 0.5182570804263416\n",
      "0\n",
      "Epoch 16 Batch 0: loss 0.6931146383285522\n",
      "4\n",
      "Epoch 16 Batch 4: loss 0.37509721517562866\n",
      "8\n",
      "Epoch 16 Batch 8: loss 0.43422359228134155\n",
      "12\n",
      "Epoch 16 Batch 12: loss 0.3242262601852417\n",
      "16\n",
      "Epoch 16 Batch 16: loss 0.5803910493850708\n",
      "20\n",
      "Epoch 16 Batch 20: loss 0.5369102954864502\n",
      "24\n",
      "Epoch 16 Batch 24: loss 0.7241644263267517\n",
      "28\n",
      "Epoch 16 Batch 28: loss 0.3267216980457306\n",
      "32\n",
      "Epoch 16 Batch 32: loss 0.3178049325942993\n",
      "36\n",
      "Epoch 16 Batch 36: loss 0.6037914752960205\n",
      "40\n",
      "Epoch 16 Batch 40: loss 0.4645030200481415\n",
      "44\n",
      "Epoch 16 Batch 44: loss 0.4677722454071045\n",
      "48\n",
      "Epoch 16 Batch 48: loss 0.410591185092926\n",
      "52\n",
      "Epoch 16 Batch 52: loss 0.4501141607761383\n",
      "56\n",
      "Epoch 16 Batch 56: loss 0.5922905802726746\n",
      "60\n",
      "Epoch 16 Batch 60: loss 0.6034348607063293\n",
      "64\n",
      "Epoch 16 Batch 64: loss 0.5826715230941772\n",
      "68\n",
      "Epoch 16 Batch 68: loss 0.6120237708091736\n",
      "72\n",
      "Epoch 16 Batch 72: loss 0.6273998022079468\n",
      "76\n",
      "Epoch 16 Batch 76: loss 0.4239305257797241\n",
      "80\n",
      "Epoch 16 Batch 80: loss 0.5065962672233582\n",
      "84\n",
      "Epoch 16 Batch 84: loss 0.5152531862258911\n",
      "88\n",
      "Epoch 16 Batch 88: loss 0.474827378988266\n",
      "92\n",
      "Epoch 16 Batch 92: loss 0.644262433052063\n",
      "96\n",
      "Epoch 16 Batch 96: loss 0.4442407488822937\n",
      "Epoch 16: loss 0.5200019197087539\n",
      "0\n",
      "Epoch 17 Batch 0: loss 0.6865543127059937\n",
      "4\n",
      "Epoch 17 Batch 4: loss 0.37282827496528625\n",
      "8\n",
      "Epoch 17 Batch 8: loss 0.4472516179084778\n",
      "12\n",
      "Epoch 17 Batch 12: loss 0.3234938383102417\n",
      "16\n",
      "Epoch 17 Batch 16: loss 0.5780346989631653\n",
      "20\n",
      "Epoch 17 Batch 20: loss 0.5348000526428223\n",
      "24\n",
      "Epoch 17 Batch 24: loss 0.733057975769043\n",
      "28\n",
      "Epoch 17 Batch 28: loss 0.33049726486206055\n",
      "32\n",
      "Epoch 17 Batch 32: loss 0.324346661567688\n",
      "36\n",
      "Epoch 17 Batch 36: loss 0.6113138198852539\n",
      "40\n",
      "Epoch 17 Batch 40: loss 0.47715049982070923\n",
      "44\n",
      "Epoch 17 Batch 44: loss 0.45797884464263916\n",
      "48\n",
      "Epoch 17 Batch 48: loss 0.42479562759399414\n",
      "52\n",
      "Epoch 17 Batch 52: loss 0.48037081956863403\n",
      "56\n",
      "Epoch 17 Batch 56: loss 0.5867259502410889\n",
      "60\n",
      "Epoch 17 Batch 60: loss 0.6081334948539734\n",
      "64\n",
      "Epoch 17 Batch 64: loss 0.5737622976303101\n",
      "68\n",
      "Epoch 17 Batch 68: loss 0.6034145951271057\n",
      "72\n",
      "Epoch 17 Batch 72: loss 0.6310792565345764\n",
      "76\n",
      "Epoch 17 Batch 76: loss 0.4252297878265381\n",
      "80\n",
      "Epoch 17 Batch 80: loss 0.5297214388847351\n",
      "84\n",
      "Epoch 17 Batch 84: loss 0.5300272107124329\n",
      "88\n",
      "Epoch 17 Batch 88: loss 0.47631222009658813\n",
      "92\n",
      "Epoch 17 Batch 92: loss 0.6463955640792847\n",
      "96\n",
      "Epoch 17 Batch 96: loss 0.4498329758644104\n",
      "Epoch 17: loss 0.5203409120074489\n",
      "0\n",
      "Epoch 18 Batch 0: loss 0.7008269429206848\n",
      "4\n",
      "Epoch 18 Batch 4: loss 0.37119001150131226\n",
      "8\n",
      "Epoch 18 Batch 8: loss 0.4329177439212799\n",
      "12\n",
      "Epoch 18 Batch 12: loss 0.3167678415775299\n",
      "16\n",
      "Epoch 18 Batch 16: loss 0.5836260318756104\n",
      "20\n",
      "Epoch 18 Batch 20: loss 0.5386033654212952\n",
      "24\n",
      "Epoch 18 Batch 24: loss 0.7261485457420349\n",
      "28\n",
      "Epoch 18 Batch 28: loss 0.32713034749031067\n",
      "32\n",
      "Epoch 18 Batch 32: loss 0.31522977352142334\n",
      "36\n",
      "Epoch 18 Batch 36: loss 0.579103410243988\n",
      "40\n",
      "Epoch 18 Batch 40: loss 0.4777514934539795\n",
      "44\n",
      "Epoch 18 Batch 44: loss 0.4563537836074829\n",
      "48\n",
      "Epoch 18 Batch 48: loss 0.40655672550201416\n",
      "52\n",
      "Epoch 18 Batch 52: loss 0.48389512300491333\n",
      "56\n",
      "Epoch 18 Batch 56: loss 0.5986537337303162\n",
      "60\n",
      "Epoch 18 Batch 60: loss 0.6088440418243408\n",
      "64\n",
      "Epoch 18 Batch 64: loss 0.5686299800872803\n",
      "68\n",
      "Epoch 18 Batch 68: loss 0.5780236124992371\n",
      "72\n",
      "Epoch 18 Batch 72: loss 0.6452949047088623\n",
      "76\n",
      "Epoch 18 Batch 76: loss 0.41579797863960266\n",
      "80\n",
      "Epoch 18 Batch 80: loss 0.5019285082817078\n",
      "84\n",
      "Epoch 18 Batch 84: loss 0.5171700716018677\n",
      "88\n",
      "Epoch 18 Batch 88: loss 0.46955618262290955\n",
      "92\n",
      "Epoch 18 Batch 92: loss 0.6344479918479919\n",
      "96\n",
      "Epoch 18 Batch 96: loss 0.43866872787475586\n",
      "Epoch 18: loss 0.5196442681812403\n",
      "0\n",
      "Epoch 19 Batch 0: loss 0.6956962943077087\n",
      "4\n",
      "Epoch 19 Batch 4: loss 0.36952510476112366\n",
      "8\n",
      "Epoch 19 Batch 8: loss 0.4303119480609894\n",
      "12\n",
      "Epoch 19 Batch 12: loss 0.32431530952453613\n",
      "16\n",
      "Epoch 19 Batch 16: loss 0.5634457468986511\n",
      "20\n",
      "Epoch 19 Batch 20: loss 0.5418106317520142\n",
      "24\n",
      "Epoch 19 Batch 24: loss 0.7148197889328003\n",
      "28\n",
      "Epoch 19 Batch 28: loss 0.32845184206962585\n",
      "32\n",
      "Epoch 19 Batch 32: loss 0.3161858022212982\n",
      "36\n",
      "Epoch 19 Batch 36: loss 0.5842827558517456\n",
      "40\n",
      "Epoch 19 Batch 40: loss 0.4719617962837219\n",
      "44\n",
      "Epoch 19 Batch 44: loss 0.45816510915756226\n",
      "48\n",
      "Epoch 19 Batch 48: loss 0.4104442596435547\n",
      "52\n",
      "Epoch 19 Batch 52: loss 0.47060057520866394\n",
      "56\n",
      "Epoch 19 Batch 56: loss 0.5686072111129761\n",
      "60\n",
      "Epoch 19 Batch 60: loss 0.5975277423858643\n",
      "64\n",
      "Epoch 19 Batch 64: loss 0.5713567137718201\n",
      "68\n",
      "Epoch 19 Batch 68: loss 0.6447480320930481\n",
      "72\n",
      "Epoch 19 Batch 72: loss 0.6152803301811218\n",
      "76\n",
      "Epoch 19 Batch 76: loss 0.4248289465904236\n",
      "80\n",
      "Epoch 19 Batch 80: loss 0.5098429322242737\n",
      "84\n",
      "Epoch 19 Batch 84: loss 0.5181607604026794\n",
      "88\n",
      "Epoch 19 Batch 88: loss 0.47775885462760925\n",
      "92\n",
      "Epoch 19 Batch 92: loss 0.6447998881340027\n",
      "96\n",
      "Epoch 19 Batch 96: loss 0.43653157353401184\n",
      "Epoch 19: loss 0.518506683409214\n"
     ]
    }
   ],
   "source": [
    "model = TrainModelFromFiles(sorted(glob.glob(f\"Models/{MODELNAME}/LRLSTMData/*\")), 20, 32, learningRate=0.001, continueTraining=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"Models/{MODELNAME}/LodeRunnerLSTMFullDataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histLSTM.weight_ih_l0   : 131072\n",
      "histLSTM.weight_hh_l0   : 65536\n",
      "histLSTM.bias_ih_l0     :   512\n",
      "histLSTM.bias_hh_l0     :   512\n",
      "colLSTM.weight_ih_l0    : 131072\n",
      "colLSTM.weight_hh_l0    : 65536\n",
      "colLSTM.bias_ih_l0      :   512\n",
      "colLSTM.bias_hh_l0      :   512\n",
      "textLSTM.weight_ih_l0   : 131072\n",
      "textLSTM.weight_hh_l0   : 65536\n",
      "textLSTM.bias_ih_l0     :   512\n",
      "textLSTM.bias_hh_l0     :   512\n",
      "infTextLSTM.weight_ih_l0: 131072\n",
      "infTextLSTM.weight_hh_l0: 65536\n",
      "infTextLSTM.bias_ih_l0  :   512\n",
      "infTextLSTM.bias_hh_l0  :   512\n",
      "outputLayer.weight      : 32768\n",
      "outputLayer.bias        :   256\n",
      "Total Params: 823552\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:<24}: {param.numel():5}\")\n",
    "    total += param.numel()\n",
    "\n",
    "print(f\"Total Params: {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DissEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
